{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SAA_FINAL_NLP_FNSA2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nMoNyGdmA6Up",
        "nBCsPpS-DRKy",
        "19prciIZxGE5",
        "mdzZ6_oSUvyP",
        "mb0ZgH-e_nSS",
        "eQQkebQIyxKn",
        "TQgxfROMwW7S",
        "yZ9F9S0YwW7T",
        "w99MV4iGpmjr"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcMnXa8f86dC"
      },
      "source": [
        "# !pip install pytorch-lightning\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWn8yyGB7_g2",
        "outputId": "0fe93f5e-8518-417c-87b4-0c5fb2393f9b"
      },
      "source": [
        "!pip install demoji\n",
        "import demoji\n",
        "demoji.download_codes()\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "# vader.VaderConstants\n",
        "# stopwords.words('english')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting demoji\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/fd/265f1ad2d745d6f46d1ede83d0054327e87154e9f14b252c1e272749e657/demoji-0.3.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from demoji) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->demoji) (3.0.4)\n",
            "Installing collected packages: colorama, demoji\n",
            "Successfully installed colorama-0.4.4 demoji-0.3.0\n",
            "Downloading emoji data ...\n",
            "... OK (Got response in 0.38 seconds)\n",
            "Writing emoji data to /root/.demoji/codes.json ...\n",
            "... OK\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-47aYSJhZtm",
        "outputId": "315d2b80-9d91-40f9-b86c-e500544deaa9"
      },
      "source": [
        "# string_with_nonASCII=\"Claim: The Ã¢â‚¬ËœFund of FundsÃ¢â‚¬â„¢ part of the #COVID19 stimulus will mobilise equity of ~Rs 50000 cr for MSMEs Fact: A similar fund set up in 2016 to mobilise Rs 60000 cr for start-ups disbursed only 6% of planned amount https://t.co/wF9lG2PM0F\"\n",
        "\n",
        "# encoded_string = string_with_nonASCII.encode(\"ascii\", \"ignore\")\n",
        "# decode_string = encoded_string.decode()\n",
        "# print(decode_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Claim: The Fund of Funds part of the #COVID19 stimulus will mobilise equity of ~Rs 50000 cr for MSMEs Fact: A similar fund set up in 2016 to mobilise Rs 60000 cr for start-ups disbursed only 6% of planned amount https://t.co/wF9lG2PM0F\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AkG8S3rwfQt"
      },
      "source": [
        "#**Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm6w8MzjwW7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd64ad3-6c33-4aa6-dcec-fa6a30ff4f7d"
      },
      "source": [
        "import gensim\n",
        "import nltk\n",
        "import numpy\n",
        "import tensorflow\n",
        "import torch\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.callbacks import *\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt \n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, LSTM, GRU, Conv1D, MaxPooling1D, Concatenate  ,SimpleRNN\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.models import Sequential \n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import glob\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.util import ngrams\n",
        "import collections\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.sentiment import vader\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMoNyGdmA6Up"
      },
      "source": [
        "## **Set The GPU:**\n",
        "Configure GPU:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9vyfi_BKA40P",
        "outputId": "343e484f-2a0a-49b7-e7eb-059100d6811c"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "device_ka_nam = tensorflow.test.gpu_device_name()\n",
        "\n",
        "if '/device:GPU:0' != device_ka_nam :\n",
        "  raise  SystemError('GPU doesn\\'t found')\n",
        "print('Found the GPU at: {}'.format(device_ka_nam))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found the GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFGXqMFhBTo7"
      },
      "source": [
        "## Observe TensorFlow speedup on GPU relative to CPU\n",
        "\n",
        "This example constructs a typical convolutional neural network layer over a\n",
        "random image and manually places the resulting ops on either the CPU or the GPU\n",
        "to compare execution speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "afLTen7yB1O6",
        "outputId": "878eeb93-615a-4239-f437-993cbd64bf56"
      },
      "source": [
        "%tensorflow_version  2.x\n",
        "import  tensorflow  as  tensorflow\n",
        "import  timeit\n",
        "\n",
        "device_ka_nam  =  tensorflow.test.gpu_device_name()\n",
        "if  device_ka_nam  !=  '/device:GPU:0':\n",
        "    \n",
        "    raise  SystemError('The GPU  device  on the syeste not  found')\n",
        "\n",
        "def  cpu():\n",
        "    with  tensorflow.device('/cpu:0'):\n",
        "        random_image_cpu  =  tensorflow.random.normal((100,  100,  100,  3))\n",
        "        net_cpu  =  tensorflow.keras.layers.Conv2D(32,  7)(random_image_cpu)\n",
        "        return  tensorflow.math.reduce_sum(net_cpu)\n",
        "\n",
        "def  gpu():\n",
        "    with  tensorflow.device('/device:GPU:0'):\n",
        "        random_image_gpu  =  tensorflow.random.normal((100,  100,  100,  3))\n",
        "        net_gpu  =  tensorflow.keras.layers.Conv2D(32,  7)(random_image_gpu)\n",
        "        return  tensorflow.math.reduce_sum(net_gpu)\n",
        "    \n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "#  Run  the  op  several  times.\n",
        "\n",
        "print('CPU  (s):')\n",
        "cpu_time  =  timeit.timeit('cpu()',  number=10,  setup=\"from  __main__  import  cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU  (s):')\n",
        "gpu_time  =  timeit.timeit('gpu()',  number=10,  setup=\"from  __main__  import  gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU  speedup  over  CPU:  {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU  (s):\n",
            "3.787136913999973\n",
            "GPU  (s):\n",
            "0.1886918040000296\n",
            "GPU  speedup  over  CPU:  20x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHtQBtTD0TnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669af3bc-e141-405a-bc54-00974367b6e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbT0vm03B610"
      },
      "source": [
        "#**Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5FzC8zg2moF"
      },
      "source": [
        "# !tar xvzf '/content/drive/MyDrive/NLP-FND/NLP-Fnd/GoogleNews-vectors-negative300.bin.gz' -C '/content/drive/MyDrive/NLP-FND/NLP-Fnd/GoogleNews-vectors-negative300.bin.gz (Unzipped Files)/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKHRWK-fwW7G"
      },
      "source": [
        "def split_train_val(x,y,ratio) :\n",
        "    in_out = []\n",
        "    \n",
        "    p_xy = [] \n",
        "    n_xy=[]\n",
        "    \n",
        "    for i in range( len(x) ) :\n",
        "        if y[i]== 0 :\n",
        "            n_xy.append( [x[i],y[i]] )\n",
        "        else :\n",
        "            p_xy.append( [ x[i] , y[i] ] )\n",
        "    \n",
        "    \n",
        "    print(\"debugging meassage -   negative \",len(n_xy)  , \" postivie \",len(p_xy))\n",
        "    \n",
        "  \n",
        "    a=random.shuffle( p_xy )\n",
        "    b=random.shuffle( n_xy )\n",
        "    print(\"debugging message  types \", type(a), type(b))\n",
        "    \n",
        "    x_val = []\n",
        "    y_val = []\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    \n",
        "    for i in range( len(n_xy )) :\n",
        "        if i < int(len(n_xy)*ratio) :\n",
        "            x_train.append( n_xy[i][0] )\n",
        "            y_train.append( n_xy[i][1] )\n",
        "        else :\n",
        "            x_val.append( n_xy[i][0] )\n",
        "            y_val.append( n_xy[i][1] )\n",
        "    \n",
        "    for i in range( len(p_xy) ) :\n",
        "        if i < int( len(p_xy) * ratio ) :\n",
        "            x_train.append( p_xy[i][0] )\n",
        "            y_train.append( p_xy[i][1] )\n",
        "        else :\n",
        "            x_val.append( p_xy[i][0] )\n",
        "            y_val.append( p_xy[i][1] )\n",
        "    \n",
        "    \n",
        "    return [x_train,y_train,x_val,y_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Et4zlgptYEB"
      },
      "source": [
        "def convertToLabels(a):\n",
        "  \n",
        "\n",
        "#Checks If the Label If Fake Or TRe On Probabilities\n",
        "    a = np.where(a > 0.5,\"real\",\"fake\" )#1, 0)\n",
        "    # print(a)\n",
        "    return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dor8xY55wW7G"
      },
      "source": [
        "# def  visualize( x , y ) :\n",
        "    \n",
        "#     t=150\n",
        "#     d=20\n",
        "    \n",
        "#     frq_n = numpy.zeros( [t] ) \n",
        "#     frq_p = numpy.zeros( [t] )\n",
        "#     frq_t = numpy.zeros( [t] )\n",
        "    \n",
        "    \n",
        "    \n",
        "#     length_list=[]\n",
        "#     lens=d\n",
        "    \n",
        "#     while(lens<=d*t) :\n",
        "#         length_list.append( lens )\n",
        "#         lens = lens + d\n",
        "#     n_p=0\n",
        "#     n_n=0\n",
        "#     print(len(length_list))\n",
        "#     for i in range( len(x) ) :\n",
        "#         length = len( x[i] )\n",
        "#         index = int(length/d)\n",
        "#         if y[i]== 0  :\n",
        "#             n_n=n_n+1\n",
        "#             frq_n[index] = frq_n[index]+1\n",
        "#         else :\n",
        "#             n_p=n_p+1\n",
        "#             frq_p[index] = frq_p[index]+1\n",
        "            \n",
        "#     frq_t = frq_p + frq_n\n",
        "         \n",
        "#     sum_n=0\n",
        "#     sum_p=0\n",
        "#     print(\"positive sample \",n_p,\" negative sample \",n_n)\n",
        "#     for i in range(t) :\n",
        "#         sum_n = sum_n + frq_n[i]\n",
        "#         sum_p = sum_p + frq_p[i]\n",
        "#         print(  i,length_list[i]  , sum_n/n_n , sum_p/n_p ,( sum_n + sum_p )/(n_n+n_p) )\n",
        "    \n",
        "#     plt.plot( length_list , frq_p , color= 'red' )\n",
        "#     plt.plot( length_list , frq_n , color = 'green' )\n",
        "#     plt.plot( length_list , frq_t , color = 'blue' )\n",
        "    \n",
        "#     plt.show()\n",
        "        \n",
        "#     return [ frq_n , frq_p , frq_t ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBCsPpS-DRKy"
      },
      "source": [
        "#Helpersss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtXeF5PADQ1C"
      },
      "source": [
        "# def count_hashTag(listOfPreProcessSent):\n",
        "#   dicFinalLists =[{\"hasTagCount\":len(re.findall('\\B#\\w\\w+',text))} for text in listOfPreProcessSent]\n",
        "#   final_df = pd.DataFrame(dicFinalLists)\n",
        "#   return final_df\n",
        "#   \"\"\"Count the Hashss Tags Heres\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# #---------------------------------------------COUNT Punctuations--------------------------------------------------\n",
        "\n",
        "# def countPnct(listOfPreProcessSent):\n",
        "#     \"\"\"Count the Pnctutaions Tags Heres\"\"\"\n",
        "#     dicFinalLists =[{\"pncICount\":len(re.findall(r'\\B[?|!]*([?!])\\B', text))} for text in listOfPreProcessSent]\n",
        "#     final_df = pd.DataFrame(dicFinalLists)\n",
        "#     return final_df\n",
        "    \n",
        "#     # min. 2 or more\n",
        "\n",
        "# # \"\"\"Mathes: :( :) :P :p :O :3 :| :/ :\\ :$ :* :@\n",
        "# # :-( :-) :-P :-p :-O :-3 :-| :-/ :-\\ :-$ :-* :-@\n",
        "# # :^( :^) :^P :^p :^O :^3 :^| :^/ :^\\ :^$ :^* :^@\n",
        "# # ): (: $: *:\n",
        "# # )-: (-: $-: *-:\n",
        "# # )^: (^: $^: *^:\n",
        "# # <3 </3 <\\3\n",
        "# # :smile: :hug: :pencil:\"\"\"    \n",
        "# def countEmoticons(text):\n",
        "#     POSITIVE = [\"*O\", \"*-*\", \"*O*\", \"*o*\", \"* *\",\n",
        "#                 \":P\", \":D\", \":d\", \":p\",\n",
        "#                 \";P\", \";D\", \";d\", \";p\",\n",
        "#                 \":-)\", \";-)\", \":=)\", \";=)\",\n",
        "#                 \":<)\", \":>)\", \";>)\", \";=)\",\n",
        "#                 \"=}\", \":)\", \"(:;)\",\n",
        "#                 \"(;\", \":}\", \"{:\", \";}\",\n",
        "#                 \"{;:]\",\n",
        "#                 \"[;\", \":')\", \";')\", \":-3\",\n",
        "#                 \"{;\", \":]\",\n",
        "#                 \";-3\", \":-x\", \";-x\", \":-X\",\n",
        "#                 \";-X\", \":-}\", \";-=}\", \":-]\",\n",
        "#                 \";-]\", \":-.)\",\n",
        "#                 \"^_^\", \"^-^\"]\n",
        "\n",
        "#     NEGATIVE = [\":(\", \";(\", \":'(\",\n",
        "#                 \"=(\", \"={\", \"):\", \");\",\n",
        "#                 \")':\", \")';\", \")=\", \"}=\",\n",
        "#                 \";-{{\", \";-{\", \":-{{\", \":-{\",\n",
        "#                 \":-(\", \";-(\",\n",
        "#                 \":,)\", \":'{\",\n",
        "#                 \"[:\", \";]\"\n",
        "#                 ]\n",
        "#     emoticon_string = r\"\"\"\n",
        "#       (?:\n",
        "#         [<>]?\n",
        "#         [:;=8]                     # eyes\n",
        "#         [\\-o\\*\\']?                 # optional nose\n",
        "#         [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
        "#         |\n",
        "#         [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "#         [\\-o\\*\\']?                 # optional nose\n",
        "#         [:;=8]                     # eyes\n",
        "#         [<>]?\n",
        "#       )\"\"\"            \n",
        "#     emoticon_re = re.compile(emoticon_string, re.VERBOSE | re.I | re.UNICODE)\n",
        "#     \"\"\"Count the Emoticons Tags Heres from Christopher regex\"\"\"\n",
        "#     emoticonList=re.findall(emoticon_re,text)\n",
        "       \n",
        "#     posList=[e for e in emoticonList if e in POSITIVE] \n",
        "#     negList=[e for e in emoticonList if e in NEGATIVE]\n",
        "#     lastOne=0\n",
        "#     # print(text.split())\n",
        "#     lastText=text.split()[-1]\n",
        "#     if len(emoticonList)>0 and lastText in emoticonList:\n",
        "#         if (emoticonList[-1]) in posList:\n",
        "#             lastOne=1#Possitive\n",
        "#         else:\n",
        "#             lastOne=-1#Negetive\n",
        "#     #\"Positive\":\"Negative\":\"Last\":\n",
        "#     return  pd.Series(np.array([len(posList),len(negList),lastOne]))\n",
        "# def countElongated(listOfPreProcessSent):\n",
        "#     \"\"\"Count the Elongateds Tags Heres\"\"\"\n",
        "\n",
        "#     dicFinalLists =[{\"elongatedCount\":len(re.findall(r'(\\w*)(\\w+)(\\2)(\\w*)', text))} for text in listOfPreProcessSent]\n",
        "#     final_df = pd.DataFrame(dicFinalLists)\n",
        "#     return final_df\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTOmi8iqLV1z"
      },
      "source": [
        "\n",
        "\n",
        "# # def findTheEMoticons(text):\n",
        "# #   exactMatch = re.compile(ur\"([^\\.]*\\bÑ‚ÑƒÑ€Ñ†Ð¸Ñ˜Ð°\\b[^\\.]*)\\.\", re.UNICODE)\n",
        "# #   print exactMatch.pattern\n",
        "# #   result= exactMatch.findall(u\"Ñ‚ÑƒÑ€Ñ†Ð¸Ñ˜Ð° Ðµ Ð½Ð° Ð²Ñ€Ð²Ð¾Ñ‚ Ð¾Ð´ Ð¸Ð½Ð´ÑƒÑÑ‚Ñ€Ð¸Ñ˜Ð°Ñ‚Ð°. Ñ‚ÑƒÑ€Ñ†Ð¸Ñ˜Ð° Ðµ Ð½Ð° Ð²Ñ€Ð²Ð¾Ñ‚ Ð¾Ð´ Ð¸Ð½Ð´ÑƒÑÑ‚Ñ€Ð¸Ñ˜Ð°Ñ‚Ð°.\")\n",
        "    \n",
        "#   # posList=len(re.findall( ru\"[\\U0001f600-\\U0001f650]\", text))\n",
        "#   # return pd.Series(np.array([len(posList)]))\n",
        "\n",
        "\n",
        "# import emoji\n",
        "\n",
        "# def extract_emojis(s):\n",
        "#   return ''.join(c for c in s if c in emoji.UNICODE_EMOJI)\n",
        "# # \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7b0CXQe91NA"
      },
      "source": [
        "# demoji.replace_with_desc(\"game is on ðŸ”¥ ðŸ”¥\", sep=\"\")#replace_with_desc(#\"game is on ðŸ”¥ ðŸ”¥\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P07r7y63EGqW"
      },
      "source": [
        "#Preprocess:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90qjfY77EJa9"
      },
      "source": [
        "#======================================================================================================\n",
        "                       \n",
        "                                       #PreProcessing\n",
        "\n",
        "#==============================================================================================\n",
        "\n",
        "def doPreProcess(text):\n",
        "  \n",
        "  # stemmer=PorterStemmer()\n",
        "  text=str(text)\n",
        "  text=demoji.replace_with_desc(text, sep=\"\")# Repalce with text\n",
        "  # text=re.sub(\"[#@]\",\" \",text)\n",
        "  # text=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())#Tags And @ Removals\n",
        "  \n",
        "  encoded_string = text.encode(\"ascii\", \"ignore\")  #For ASCII Onlys\n",
        "  text = encoded_string.decode()\n",
        "\n",
        "#Eiether-------------------------------------------\n",
        "  # text=re.sub(\"([@#][A-Za-z0-9]+)\",\"\",text)#Replace by space those words with # and @\n",
        "\n",
        "      #Or\n",
        "  text=re.sub(\"[@#]\",\"\",text)#Replace by space the chars as  # and @\n",
        "#-----------------------------------------------------------\n",
        "\n",
        "  text=re.sub(r\"http\\S+\", \"\", text)#Remove the Links\n",
        " \n",
        "  \n",
        "  #Split Lower StopWordsRemoval stemming\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  words=tokenizer.tokenize(text.lower())#Tokenize and casings\n",
        "  # words=text.lower().split()\n",
        "  words=[word for word in words ]#if word not in set(stopwords.words('english'))]\n",
        "  return ' '.join(words) \n",
        "\n",
        "# def preProcessing(df):\n",
        "#   df=df.filter(\"tweet\")[\"tweet\"].apply(doPreProcess)\n",
        "#   return df\n",
        "\n",
        "#======================================================================================================\n",
        "                       \n",
        "                                       #Global Data Frames\n",
        "\n",
        "#==============================================================================================\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o38tvTLudsu2"
      },
      "source": [
        "# text=\"#Arnab Fights @Back\"\n",
        "\n",
        "\n",
        "# print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu252Na0wW7G"
      },
      "source": [
        "#**loading pretrained word2vec  ' trained by negative subsampling '**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vsixdhrwW7G"
      },
      "source": [
        "#________________________________________________________Pretrained WOrd To Vec Loading:___________________________________________________--------------------------------\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "filename = '/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Unzipped/GoogleNews-vectors-negative300.bin'\n",
        "# word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/NLP-FND/NLP-Fnd/Unzipped/GoogleNews-vectors-negative300.bin', binary=True) \n",
        "word2vec = KeyedVectors.load_word2vec_format(filename, binary=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19prciIZxGE5"
      },
      "source": [
        "#**Data Augmentation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6m9LolfFxnT"
      },
      "source": [
        "#\n",
        "########################################################################################################################\n",
        "#   _____________________________________________Augmentation________________________________________________________   #\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "# options = [0,1,2,3,4]\n",
        "# def replacement(word) :\n",
        "# \tsimilars = word2vec.most_similar(positive=[word] , topn=5)  #model.most_similar(positive=[word] , topn=5) \n",
        "# \tchoice = random.choice( options )\n",
        "# \treturn similars[choice][0] \n",
        "# replacement('missiles')\n",
        "# #----------------------------------------------------------------------------------------------\n",
        "# def Edits(sentence):\n",
        "# \tindexs = []\n",
        "# \tfor i in range(len(sentence)):\n",
        "# \t\tindexs.append(i) \n",
        "# \twhile len(indexs) > 0 :\n",
        "# \t\tindex = random.choice(indexs) \n",
        "# \t\ttarget_word = sentence[index]\n",
        "# \t\ttry :\n",
        "# \t\t\tnew_word = replacement(target_word)\n",
        "# \t\t\treturn [target_word, new_word] \n",
        "\n",
        "# \t\texcept :\n",
        "# \t\t\tindexs.remove(index) \n",
        "# \treturn ['','']\n",
        "\n",
        "# #------------------------------------------------DATA AUGMENTATION FOR TRUE DATA-SET----------------------------------------------\n",
        "\n",
        "# import pandas\n",
        "# for i in range(200) :\n",
        "#   L1 = []\n",
        "#   L2 = []\n",
        "#   stat_title = 0\n",
        "#   stat_text = 0\n",
        "#   pd_o = pandas.read_csv('/content/drive/My Drive/NLP-Fnd/NLP-Fnd/true_folder/train_'+str(i)+'.csv')\n",
        "#   print( \"folder  : \",i , \"   size : \",pd_o.shape[0] )\n",
        "#   for j in range( pd_o.shape[0] ) :\n",
        "#     l  = pd_o['title'][j].lower().split(' ')\n",
        "#     [old,new] = Edits( l )\n",
        "#     #print(old,new)\n",
        "#     if old=='' or new=='' :\n",
        "#       L1.append( pd_o['title'][j] )\n",
        "#       #print(\"no\")\n",
        "#     else :\n",
        "#       #print(\"yes\")\n",
        "#       aug_str = '' \n",
        "#       stat_title  = stat_title + 1\n",
        "#       for k in range( len(l) ) :\n",
        "#         #print(l[k])\n",
        "#         if l[k] == old :\n",
        "#           aug_str = aug_str + ' ' + new \n",
        "#         else :\n",
        "#           aug_str = aug_str + ' ' + l[k]\n",
        "#         #print(aug_str)\n",
        "#       L1.append( aug_str )\n",
        "#     l = pd_o['text'][j].lower().split(' ') \n",
        "#     [old,new] = Edits( l )\n",
        "#     #print(old,new)\n",
        "#     if old=='' or new==''  :\n",
        "#       L2.append( pd_o['text'][j] )\n",
        "#     else :\n",
        "#       aug_str = ''\n",
        "#       stat_text = stat_text + 1\n",
        "#       for k in range( len(l) ) :\n",
        "#         if l[k] == old :\n",
        "#           aug_str = aug_str + ' ' + new \n",
        "#         else :\n",
        "#           aug_str = aug_str + ' ' + l[k]\n",
        "#       L2.append( aug_str )\n",
        "#   pd_a = pandas.DataFrame()\n",
        "#   pd_a['title']  = L1 \n",
        "#   pd_a['text'] = L2\n",
        "#   print(\"saving \",i,\"  :   stats \",stat_title,stat_text)\n",
        "#   pd_a.to_csv('/content/drive/My Drive/NLP-Fnd/NLP-Fnd/true_aug_w2v/'+\"train_\"+str(i)+\".csv\")\n",
        "\n",
        "\n",
        "\n",
        "# #-------------------------------------------------DATA AUGMENTATION FOR FAKE DATA-SET-----------------------------------------------\n",
        "# for i in range(200) :\n",
        "#   L1 = []\n",
        "#   L2 = []\n",
        "#   stat_title = 0\n",
        "#   stat_text = 0\n",
        "#   pd_o = pandas.read_csv('/content/drive/My Drive/NLP-Fnd/NLP-Fnd/fake_folder/train_'+str(i)+'.csv')\n",
        "#   print( \"folder  : \",i , \"   size : \",pd_o.shape[0] )\n",
        "#   for j in range( pd_o.shape[0] ) :\n",
        "#     l  = pd_o['title'][j].lower().split(' ')\n",
        "#     [old,new] = Edits( l )\n",
        "#     #print(old,new)\n",
        "#     if old=='' or new=='' :\n",
        "#       L1.append( pd_o['title'][j] )\n",
        "#       #print(\"no\")\n",
        "#     else :\n",
        "#       #print(\"yes\")\n",
        "#       aug_str = '' \n",
        "#       stat_title  = stat_title + 1\n",
        "#       for k in range( len(l) ) :\n",
        "#         #print(l[k])\n",
        "#         if l[k] == old :\n",
        "#           aug_str = aug_str + ' ' + new \n",
        "#         else :\n",
        "#           aug_str = aug_str + ' ' + l[k]\n",
        "#         #print(aug_str)\n",
        "#       L1.append( aug_str )\n",
        "#     l = pd_o['text'][j].lower().split(' ') \n",
        "#     [old,new] = Edits( l )\n",
        "#     #print(old,new)\n",
        "#     if old=='' or new==''  :\n",
        "#       L2.append( pd_o['text'][j] )\n",
        "#     else :\n",
        "#       aug_str = ''\n",
        "#       stat_text = stat_text + 1\n",
        "#       for k in range( len(l) ) :\n",
        "#         if l[k] == old :\n",
        "#           aug_str = aug_str + ' ' + new \n",
        "#         else :\n",
        "#           aug_str = aug_str + ' ' + l[k]\n",
        "#       L2.append( aug_str )\n",
        "#   pd_a = pandas.DataFrame()\n",
        "#   pd_a['title']  = L1 \n",
        "#   pd_a['text'] = L2\n",
        "#   print(\"saving \",i,\"  :   stats \",stat_title,stat_text)\n",
        "#   pd_a.to_csv('/content/drive/My Drive/NLP-Fnd/NLP-Fnd/fake_aug_w2v/'+\"train_\"+str(i)+\".csv\")\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------------END--------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#______________________________________________________________CSV FOLDER FETCH_______________________________________________________\n",
        "\n",
        "#MISCSS:\n",
        "###return str.replace(/\\s/g, '');\n",
        "\n",
        "# \\b\\s\\b\n",
        "\n",
        "#Replace(\"   \",\" \")\n",
        "\n",
        "#-------------------------------------------------\n",
        "\n",
        "#PAth To Fetch Mltiple CSV's\n",
        "# def changeToSIngleDataframe(path):\n",
        "#     all_files = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "#     li = []\n",
        "\n",
        "#     for filename in all_files:\n",
        "#           if (filename.find(\"test\") != -1):\n",
        "#             continue\n",
        "#           df = pd.read_csv(filename, index_col=None, header=0)\n",
        "#           li.append(df)\n",
        "          \n",
        "#     frame = pd.concat(li, axis=0, ignore_index=True)\n",
        "#     return frame\n",
        "\n",
        "\n",
        "# frame_true=changeToSIngleDataframe(r'/content/drive/MyDrive/NLP-FND/NLP-Fnd/true_folder')\n",
        "# print(frame_true.shape)\n",
        "# frame_true_ag=changeToSIngleDataframe(r'/content/drive/MyDrive/NLP-FND/NLP-Fnd/true_aug_w2v')\n",
        "# print(frame_true_ag.shape)\n",
        "# frame_false=changeToSIngleDataframe(r'/content/drive/MyDrive/NLP-FND/NLP-Fnd/fake_folder')\n",
        "# print(frame_false.shape)\n",
        "# frame_false_ag=changeToSIngleDataframe(r'/content/drive/MyDrive/NLP-FND/NLP-Fnd/fake_aug_w2v')\n",
        "# print(frame_false_ag.shape)\n",
        "# #-------------------------------COMBINEE ALL FRAMES------------------------------------\n",
        "\n",
        "\n",
        "# frame_true[\"Label\"]=1\n",
        "# frame_true_ag[\"Label\"]=1\n",
        "# frame_false[\"Label\"]=0\n",
        "# frame_false_ag[\"Label\"]=0\n",
        "# train_data=frame_true.append(frame_true_ag).append(frame_false).append(frame_false_ag)\n",
        "# train_data=train_data.filter([\"title\",\"text\",\"date\",\"subject\",\"Label\"])\n",
        "#Save To CSV Augmented\n",
        "# train_data.to_csv(\"/content/drive/MyDrive/NLP-FND/NLP-Fnd/Final_Aug_Original.csv\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgO5r5WR1qt-"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZjzTLPixTYR"
      },
      "source": [
        "#**Main Code Formations:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAFP1MR4wW7H"
      },
      "source": [
        "getting words for which embedding is present"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-BTjsswxziE"
      },
      "source": [
        "#**Declarations**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1BzOs7vwW7H",
        "outputId": "eb50958b-1851-4c26-efb1-84c394792a50"
      },
      "source": [
        "\n",
        "#\n",
        "########################################################################################################################\n",
        "#_____________________________________________________Vocab Extracting From WordToVec  of type Dictionary Keys___________________________________________ \n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "vocab_word2vec = word2vec.wv.vocab.keys()\n",
        "\n",
        "#Getting the Vocab as Tokenized to List form   __________________TOKEN LIST HERE________________________\n",
        "\n",
        "all_data_tokens=list(vocab_word2vec)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82prS6W_Ybht"
      },
      "source": [
        "\n",
        "#**Either:.......................................Train Data Fetch   &    TRAIN TEST SPLIT :**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67VJibMYBoGH"
      },
      "source": [
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "#__________________________________________Loading The Training Dataa in dfs:____________________________________________________\n",
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "# train_data_Fake=pd.read_csv(\"/content/drive/MyDrive/NLP-FND/NLP-Fnd/Fake.csv\") #FAKE File Read\n",
        "test_data=pd.read_csv(\"/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Codalab/Constraint_English_Test - Sheet1.csv\")\n",
        "\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Codalab/Constraint_English_Train - Sheet1.csv\")\n",
        "\n",
        "\n",
        "val_data = pd.read_csv(\"/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Codalab/Constraint_English_Val - Sheet1.csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# /content/drive/MyDrive/NLP-FND/NLP-Fnd/Codalab/Constraint_English_Train - Sheet1.csv\n",
        "\n",
        "# train_data_Tre=pd.read_csv(\"/content/drive/MyDrive/NLP-FND/NLP-Fnd/True.csv\") #TRUE File Read\n",
        "#Columns---->title\ttext\tsubject\tdate\n",
        "\n",
        "#___________LABEL ASSIGN__________________\n",
        "# train_data_Fake[\"Label\"]=0 #Fake Label\n",
        "# train_data_Tre[\"Label\"]=1  #True Label \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Title  Body  Combine\n",
        "# ___________________________Concats two DatafRames of True and Fake to One______________________\n",
        "\n",
        "\n",
        "\n",
        "#Creating the text and title in one column concat\n",
        "#train_data[\"combined\"] = train_data[\"title\"] +\" \"+train_data[\"text\"]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u7qwuUHIxKQ"
      },
      "source": [
        "result_df1=train_data[[\"tweet\"]][\"tweet\"].apply(doPreProcess)#REmoving The Stopwords And Punctuations and lower case \n",
        "result_df2=test_data[[\"tweet\"]][\"tweet\"].apply(doPreProcess)#REmoving The Stopwords And Punctuations and lower case \n",
        "result_df3=val_data[[\"tweet\"]][\"tweet\"].apply(doPreProcess)#REmoving The Stopwords And Punctuations and lower case "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8T9E_w672rT"
      },
      "source": [
        "train_data[\"preprocess\"]=result_df1\n",
        "test_data[\"preprocess\"]=result_df2\n",
        "val_data[\"preprocess\"]=result_df3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C_7s_uK773i"
      },
      "source": [
        "train_data.to_csv(\"/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Codalab/train_preprocess_final.csv\",index=False)\n",
        "test_data.to_csv(\"/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Codalab/test_preprocess_final.csv\",index=False)\n",
        "val_data.to_csv(\"/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Codalab/val_preprocess_final.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaryxwBHKVYE"
      },
      "source": [
        "test_data=pd.read_csv(\"/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Codalab/test_preprocess_final.csv\")\n",
        "\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Codalab/train_preprocess_final.csv\")\n",
        "\n",
        "\n",
        "val_data = pd.read_csv(\"/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Codalab/val_preprocess_final.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "heLbFdJON_X7",
        "outputId": "2be30358-6279-491b-cd8f-6856fe6acd1f"
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>preprocess</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Our daily update is published. States reported...</td>\n",
              "      <td>our daily update is published states reported ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Alfalfa is the only cure for COVID-19.</td>\n",
              "      <td>alfalfa is the only cure for covid 19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>President Trump Asked What He Would Do If He W...</td>\n",
              "      <td>president trump asked what he would do if he w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>States reported 630 deaths. We are still seein...</td>\n",
              "      <td>states reported 630 deaths we are still seeing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>This is the sixth time a global health emergen...</td>\n",
              "      <td>this is the sixth time a global health emergen...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                         preprocess\n",
              "0   1  ...  our daily update is published states reported ...\n",
              "1   2  ...              alfalfa is the only cure for covid 19\n",
              "2   3  ...  president trump asked what he would do if he w...\n",
              "3   4  ...  states reported 630 deaths we are still seeing...\n",
              "4   5  ...  this is the sixth time a global health emergen...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4-Bw-QYVAWC"
      },
      "source": [
        "#________________________________________________________________________APPENDING A LINEAR TYPE SERIES______________________________________ \n",
        "# features=pd.concat([train_data[\"title\"], train_data[\"text\"], train_data[\"combined\"]], ignore_index=True)\n",
        "# labels=pd.concat([train_data[\"Label\"],train_data[\"Label\"],train_data[\"Label\"]], ignore_index=True)\n",
        "# labels=pd.concat([train_data[\"Label\"],train_data[\"Label\"],train_data[\"Label\"]], ignore_index=True)\n",
        "# labels=pd.concat([train_data[\"Label\"],train_data[\"Label\"],train_data[\"Label\"]], ignore_index=True)\n",
        "# labels=pd.concat([train_data[\"label\"]], ignore_index=True)\n",
        "\n",
        "\n",
        "#80 Per Val And 20 Per Test\n",
        "# [x_train_val,y_train_val,x_test,y_test] = split_train_val( all_data_x,all_data_y,0.8)#x_train_val , y_train_val , 0.7 )\n",
        "\n",
        "#Ot Of 80% Val 70 % Percent Train And Remaining 30% As Core Validation\n",
        "# [x_train,y_train,x_val,y_val] = split_train_val( x_train_val,y_train_val,0.7)#x_train_va\n",
        "\n",
        "\n",
        "# y_test=pd.Series(y_test)\n",
        "# y_train=pd.Series(y_train)\n",
        "# y_val=pd.Series(y_val)\n",
        "\n",
        "\n",
        "#__________________________________________________________________________________________________\n",
        "#Train\n",
        "features=pd.concat([train_data[\"preprocess\"]], ignore_index=True)\n",
        "labels=pd.concat([train_data[\"label\"]], ignore_index=True)\n",
        "\n",
        "#Val\n",
        "features_val=pd.concat([val_data[\"preprocess\"]], ignore_index=True)\n",
        "labels_val=pd.concat([val_data[\"label\"]], ignore_index=True)\n",
        "\n",
        "#test\n",
        "features_test=pd.concat([test_data[\"preprocess\"]], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "#_________________________________________________________TRAIN AND TEST___________________________________\n",
        "\n",
        "# all_data_x=features#[]#1. Title  2. Body  3. Combine \n",
        "# all_data_y=labels#[]#Fake 0 ,True 1\n",
        "\n",
        "x_test=features_test\n",
        "\n",
        "x_train=features\n",
        "y_train=labels\n",
        "\n",
        "x_val=features_val\n",
        "y_val=labels_val\n",
        "\n",
        "\n",
        "#==================================================================\n",
        "final_df=pd.concat([train_data,val_data])\n",
        "from sklearn.model_selection import train_test_split\n",
        "final_df.drop_duplicates(['preprocess','label'],inplace=True)\n",
        "\n",
        "x_train_val=pd.concat([final_df[\"preprocess\"]], ignore_index=True)\n",
        "y_train_val=pd.concat([final_df[\"label\"]], ignore_index=True)\n",
        "\n",
        "# [x_train,y_train,x_val,y_val] = split_train_val( x_train_val,y_train_val,0.8)\n",
        "x_train, x_val, y_train, y_val= train_test_split(x_train_val, y_train_val, test_size=0.2, random_state=42)\n",
        "#======================================================================\n",
        "education = {'fake':0.0, 'real':1.0}\n",
        "listOfdata1=y_train    #.tolist()\n",
        "# listOfdata2=y_test.tolist()\n",
        "listOfdata3=y_val     #.tolist()\n",
        "\n",
        "num_ed_data = [education[i] for i in listOfdata1]\n",
        "# num_ed_data2 = [education[i] for i in listOfdata2]\n",
        "num_ed_data3 = [education[i] for i in listOfdata3]\n",
        "\n",
        "\n",
        "# y_test=pd.Series(num_ed_data2)\n",
        "y_train=pd.Series(num_ed_data)\n",
        "y_val=pd.Series(num_ed_data3)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5vtiWA_NRR-",
        "outputId": "f6cb1d39-cbf2-42fe-f8d2-324944bded33"
      },
      "source": [
        "# len(val_data[\"preprocess\"])#.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2140"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApUCB7CQBgBN"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF3vmUDqBzPp",
        "outputId": "b2062003-e30d-4dc7-cfa2-43c98d64a4de"
      },
      "source": [
        "# len(final_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8395"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdzZ6_oSUvyP"
      },
      "source": [
        "#**Comments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IX6L4YUERvj"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())#Tags And @ Removals\n",
        "\n",
        "\n",
        "#re.sub(r\"http\\S+\", \"\", subject)#Remove the Links\n",
        "\n",
        "\n",
        "# import emoji\n",
        "# text = \"game is on ðŸ”¥ ðŸ”¥\"\n",
        "# emoji.demojize(text, delimiters=(\"\", \"\"))  # 'game is on fire fire'\n",
        "\n",
        "# dfPre=preProcessing(dfNew).to_frame(\"preprocess\") \n",
        "\n",
        "# listOfPreProcessSent=dfPre[\"preprocess\"].tolist()\n",
        "\n",
        "# listOfOriginal=dfNew.head(1000)[\"5\"].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW78FGMjD25u"
      },
      "source": [
        "# featre1=train_data.head(1000)\n",
        "# featre1=test_data[\"5\"].apply(countEmoticons)\n",
        "# featre1.columns=[\"Positive\",\"Negative\",\"Last\"]\n",
        "# print(featre1.shape[0])\n",
        "# featre1.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9QP7Dy2Iibe"
      },
      "source": [
        "# strings=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",\"Hello #Jazz Hi This is Lokesh\").split())#Tags And @ Removals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLZEoChlIr5n",
        "outputId": "04c59118-39e7-42d0-ca84-5c0d42e8c7b4"
      },
      "source": [
        "# =result_df[\"tweet\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello Jazz Hi This is Lokesh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "bfohtCk5HgqD",
        "outputId": "33bfa4bf-e7e2-485b-b4cc-79c6f0e942e4"
      },
      "source": [
        "# train_data[\"preprocess\"]=result_df\n",
        "\n",
        "# result_df=train_data[[\"tweet\"]][\"tweet\"].apply(countEmoticons)#REmoving The Stopwords And Punctuations and lower case \n",
        "# result_df[result_df[2]>0].head(20)\n",
        "\n",
        "\n",
        "# \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [0, 1, 2]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "v-S2NTjuJz8T",
        "outputId": "85e2c128-694e-4db3-dae2-50c167911a52"
      },
      "source": [
        "# train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>preprocess</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
              "      <td>real</td>\n",
              "      <td>cdc current report 99031 death gener discrep d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>States reported 1121 deaths a small rise from ...</td>\n",
              "      <td>real</td>\n",
              "      <td>state report 1121 death small rise last tuesda...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
              "      <td>fake</td>\n",
              "      <td>polit correct woman almost use pandem excus re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
              "      <td>real</td>\n",
              "      <td>indiafightscorona 1524 covid test laboratori i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Populous states can generate large case counts...</td>\n",
              "      <td>real</td>\n",
              "      <td>popul state gener larg case count look new cas...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                         preprocess\n",
              "0   1  ...  cdc current report 99031 death gener discrep d...\n",
              "1   2  ...  state report 1121 death small rise last tuesda...\n",
              "2   3  ...  polit correct woman almost use pandem excus re...\n",
              "3   4  ...  indiafightscorona 1524 covid test laboratori i...\n",
              "4   5  ...  popul state gener larg case count look new cas...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziK_aqByHvrP",
        "outputId": "ac41b3a3-c76e-4b1a-c309-ce2e699c2e91"
      },
      "source": [
        "# train_data[[\"tweet\"]][\"tweet\"].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    The CDC currently reports 99031 deaths. In gen...\n",
              "1    States reported 1121 deaths a small rise from ...\n",
              "2    Politically Correct Woman (Almost) Uses Pandem...\n",
              "3    #IndiaFightsCorona: We have 1524 #COVID testin...\n",
              "4    Populous states can generate large case counts...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP7i7IWSioPL",
        "outputId": "0486e429-d1ba-40b2-abcb-560216e41d54"
      },
      "source": [
        "# test_data.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2140"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "BW7CdslCecb1",
        "outputId": "e14c0134-00b1-4667-9dca-3cc0d4d24a28"
      },
      "source": [
        "# train_data.loc[6][\"tweet\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'If you tested positive for #COVID19 and have no symptoms stay home and away from other people. Learn more about CDCâ€™s recommendations about when you can be around others after COVID-19 infection: https://t.co/z5kkXpqkYb. https://t.co/9PaMy0Rxaf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgDFyMUqedkc"
      },
      "source": [
        "# a = numpy.zeros( 150 )\n",
        "# lg=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVrSQ7MyeiZP"
      },
      "source": [
        "# for i in range( train_data.shape[0] ) :\n",
        "#   if len(train_data['tweet'][i].split(' ')) < 150 :\n",
        "#     a[ len( train_data['tweet'][i].split(' ') ) ] = a[ len(train_data['tweet'][i].split(' ') ) ] + 1 \n",
        "#   else :\n",
        "#     lg = lg+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjaTPx-kgKq2",
        "outputId": "921611ef-4d8d-4583-c917-ee00d0e494fb"
      },
      "source": [
        "# lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSyH1clcfMP1"
      },
      "source": [
        "# x1=[]\n",
        "# for x in range(150) :\n",
        "#   x1.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pC8Ocx_fWX7"
      },
      "source": [
        "# from matplotlib import pyplot "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "4c6noI6KfmVg",
        "outputId": "5e30c67e-4853-4b92-8daa-1ec7ad9cde07"
      },
      "source": [
        "# pyplot.plot(x1,a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5b7dfec208>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2mX7iYObp3k"
      },
      "source": [
        "# train_data.groupby('tweet')['tweet'].apply(lambda x: np.mean(len(x)))\n",
        "# tweet\n",
        "# \"'Akiraâ€™ is a 1988 movie about an apocalyptic event taking place months before Tokyo 2020 Olympics showing the WHO advising Japan to postpone the Olympics because of a pandemic risk.\"                                                                                               1.0\n",
        "# \"93% of the (COVID-19) cases in the state of Illinois have come from Chicago.\"                                                                                                                                                                                                        1.0\n",
        "# \"99%\" of coronavirus cases \"are totally harmless.\"                                                                                                                                                                                                                                    1.0\n",
        "# \"@Perchbicester\" ðŸ—£ï¸ Want to get the team together? We have COVID-safe meeting rooms and plenty of #coworking space, free WiFi and coffee. Conveniently located off the M40 in #bicester, close to train, road and bus links https://t.co/O5dPcZVeFU https://t.co/s1WaW2iD1u           1.0\n",
        "# \"As NY continues to fight the pandemic, we want to make sure NYers still struggling financially will not be forced from their homes as a result of COVID,â€ Cuomo said.\\nWhen snow's on the ground ok to evict? https://t.co/D4bspug0cD                                                1.0\n",
        "#                                                                                                                                                                                                                                                                                      ... \n",
        "# ðŸ—£ï¸ Want to get the team together? We have COVID-safe meeting rooms and plenty of #coworking space, free WiFi and coffee. Conveniently located off the M40 in #bicester, close to train, road and bus links https://t.co/MEsNAXYVFW https://t.co/60l0c1eG4V                            1.0\n",
        "# ðŸ˜· More evidence that universal masking is critically important to slow the spread of the virus. https://t.co/g0mi6wr79P #covid19                                                                                                                                                      1.0\n",
        "# ðŸš“ Criminal inquiry launched into allegations of poor maternity standards at an NHS trust. https://t.co/Y7lYfCEln2                                                                                                                                                                     1.0\n",
        "# ðŸš¥ Ready. Set. Go! The race to create a COVID-19 vaccine is on. https://t.co/HKIWsG6nKR https://t.co/XHBeRSr9vK                                                                                                                                                                        1.0\n",
        "# ðŸš¨ A game changer for COVID testing Abbott Labs' is working on a $5 antigen test that delivers results in 15 minutes. Their BinaxNOW card &amp; app will provide negative testers with a date-stamped health pass that is renewed each time they test. @CNN https://t.co/7mLQkz2AnE    1.0\n",
        "# Name: tweet, Length: 6420, dtype: float64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yh-jdlqbbn2",
        "outputId": "fa7e87cb-e38c-4f94-df05-1a758630fed1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "debugging meassage -   negative  0  postivie  6420\n",
            "debugging message  types  <class 'NoneType'> <class 'NoneType'>\n",
            "debugging meassage -   negative  0  postivie  5136\n",
            "debugging message  types  <class 'NoneType'> <class 'NoneType'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb0ZgH-e_nSS"
      },
      "source": [
        "#**OR.......................................................Augment CSV Read(Heres For Project)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUwG2kpk_kur",
        "outputId": "926c5586-b955-4f7a-e88a-65b49c037999"
      },
      "source": [
        "\n",
        "# train_data=pd.read_csv(\"/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Final_Aug_Original.csv\")\n",
        "# #Creating the text and title in one column concat\n",
        "# train_data[\"combined\"] = train_data[\"title\"] +\" \"+train_data[\"text\"]\n",
        "\n",
        "\n",
        "\n",
        "# #________________________________________________________________________APPENDING A LINEAR TYPE SERIES______________________________________ \n",
        "# # features=pd.concat([train_data[\"title\"], train_data[\"text\"], train_data[\"combined\"]], ignore_index=True)\n",
        "# features=pd.concat([train_data[\"text\"]], ignore_index=True)\n",
        "# # labels=pd.concat([train_data[\"Label\"],train_data[\"Label\"],train_data[\"Label\"]], ignore_index=True)\n",
        "# labels=pd.concat([train_data[\"Label\"]], ignore_index=True)\n",
        "\n",
        "\n",
        "# #_________________________________________________________SPLIT INTO TRAIN AND TEST___________________________________\n",
        "\n",
        "# all_data_x=features#[]#1. Title  2. Body  3. Combine \n",
        "# all_data_y=labels#[]#Fake 0 ,True 1\n",
        "\n",
        "\n",
        "# #TODO:GPU Learning\n",
        "\n",
        "# #80 Per Val And 20 Per Test\n",
        "# [x_train_val,y_train_val,x_test,y_test] = split_train_val( all_data_x,all_data_y,0.8)#x_train_val , y_train_val , 0.7 )\n",
        "\n",
        "# #Ot Of 80% Val 70 % Percent Train And Remaining 30% As Core Validation\n",
        "# [x_train,y_train,x_val,y_val] = split_train_val( x_train_val,y_train_val,0.7)#x_train_va\n",
        "\n",
        "# y_test=pd.Series(y_test)\n",
        "# y_train=pd.Series(y_train)\n",
        "# y_val=pd.Series(y_val)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "debugging meassage -   negative  40000  postivie  40000\n",
            "debugging message  types  <class 'NoneType'> <class 'NoneType'>\n",
            "debugging meassage -   negative  32000  postivie  32000\n",
            "debugging message  types  <class 'NoneType'> <class 'NoneType'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkhGCCgJjGWo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQQkebQIyxKn"
      },
      "source": [
        "#**Commmented Part**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtcAuaEBYX7B"
      },
      "source": [
        "# train_data_Fake[\"Label\"]=0 #Fake Label\n",
        "# train_data_Tre[\"Label\"]=1  #True Label \n",
        "\n",
        "\n",
        "\n",
        "# #Title  Body  Combine\n",
        "# # ___________________________COncatInating two DatafRames of True and Fake______________________\n",
        "\n",
        "# train_data = train_data_Fake.append([train_data_Tre])\n",
        "# #Creating the text and title in one column concat\n",
        "# train_data[\"combined\"] = train_data[\"title\"] +\" \"+train_data[\"text\"]\n",
        "\n",
        "\n",
        "# # # result = pd.concat([df1, s1], axis=1)\n",
        "# # #20%\n",
        "# # test_data=pandas.read_csv(\"\")\n",
        "\n",
        "# #Testing The Datas_____________________________________FINAL TRAIN DATA FORMED HERE_________________________________________________\n",
        "# train_data.head()[\"combined\"][0]\n",
        "# train_data_Tre.head()\n",
        "\n",
        "\n",
        "# from loader import stanford_train , stanford_test\n",
        "# print(\"details about training and validation sets combined\")\n",
        "# [x_train_val,y_train_val,c] = stanford_train()\n",
        "# print(\"details on test set\")\n",
        "# [x_test,y_test,c] =  stanford_test()\n",
        "# all_data = x_train_val + x_test\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i2AFlldy5MK"
      },
      "source": [
        "#**FITTING TO TOKENIZER:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq8KU5IGwW7M",
        "outputId": "b2794cc2-e515-4f4e-fd3e-c21367302b9d"
      },
      "source": [
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "#_____________________________________________FIT INTO TOKENIZER WORDTOVEC VOCAB_________________________________________________\n",
        "\n",
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts(list(all_data))\n",
        "\n",
        "tokenizer.fit_on_texts(list(all_data_tokens))\n",
        "\n",
        "#_______________________________________________Getting The Size Of WOrd Vocab____________________________________________________\n",
        "size_of_vocabulary_dataset=len(tokenizer.word_index) + 1 #+1 for padding\n",
        "print(size_of_vocabulary_dataset)\n",
        "\n",
        "#_________________TEST_______________\n",
        "tokenizer.word_index\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "773165\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'com': 1,\n",
              " 'www': 2,\n",
              " 'by': 3,\n",
              " 'al': 4,\n",
              " 'inc': 5,\n",
              " 'nasdaq': 6,\n",
              " 'de': 7,\n",
              " 'the': 8,\n",
              " 'http': 9,\n",
              " 'dr': 10,\n",
              " 'r': 11,\n",
              " 'minister': 12,\n",
              " 'visit': 13,\n",
              " 'la': 14,\n",
              " 'van': 15,\n",
              " 'nyse': 16,\n",
              " 'of': 17,\n",
              " 'spokesman': 18,\n",
              " 'Â®': 19,\n",
              " 'to': 20,\n",
              " 'm': 21,\n",
              " 'e': 22,\n",
              " 'corporation': 23,\n",
              " 'n': 24,\n",
              " 's': 25,\n",
              " 'corp': 26,\n",
              " 'org': 27,\n",
              " 'david': 28,\n",
              " 'spokeswoman': 29,\n",
              " 'and': 30,\n",
              " 'coach': 31,\n",
              " 'co': 32,\n",
              " 'mark': 33,\n",
              " 'john': 34,\n",
              " 'd': 35,\n",
              " 'peter': 36,\n",
              " 'a': 37,\n",
              " 'c': 38,\n",
              " 'tm': 39,\n",
              " 'ltd': 40,\n",
              " 'el': 41,\n",
              " 'michael': 42,\n",
              " 'paul': 43,\n",
              " 'mike': 44,\n",
              " 'chief': 45,\n",
              " 'steve': 46,\n",
              " 'net': 47,\n",
              " 'technologies': 48,\n",
              " 'mr': 49,\n",
              " 'tsx': 50,\n",
              " 'tom': 51,\n",
              " 'group': 52,\n",
              " 'in': 53,\n",
              " 'x': 54,\n",
              " 'news': 55,\n",
              " 'chris': 56,\n",
              " 'p': 57,\n",
              " 'b': 58,\n",
              " 'joe': 59,\n",
              " \"'s\": 60,\n",
              " 'l': 61,\n",
              " 'quote': 62,\n",
              " 'jim': 63,\n",
              " 'robert': 64,\n",
              " 'systems': 65,\n",
              " 'singh': 66,\n",
              " 'road': 67,\n",
              " 'dan': 68,\n",
              " 'v': 69,\n",
              " 'j': 70,\n",
              " 'matt': 71,\n",
              " 'jeff': 72,\n",
              " 't': 73,\n",
              " 'st': 74,\n",
              " 'richard': 75,\n",
              " 'g': 76,\n",
              " \"d'\": 77,\n",
              " 'for': 78,\n",
              " 'director': 79,\n",
              " 'bob': 80,\n",
              " 'â„¢': 81,\n",
              " 'eric': 82,\n",
              " 'international': 83,\n",
              " 'andrew': 84,\n",
              " 'brian': 85,\n",
              " 'park': 86,\n",
              " 'president': 87,\n",
              " 'kevin': 88,\n",
              " 'scott': 89,\n",
              " 'staff': 90,\n",
              " 'writer': 91,\n",
              " 'james': 92,\n",
              " 'frank': 93,\n",
              " 'pa': 94,\n",
              " 'otcbb': 95,\n",
              " 'bill': 96,\n",
              " 'ali': 97,\n",
              " 'chairman': 98,\n",
              " 'abdul': 99,\n",
              " 'energy': 100,\n",
              " 'del': 101,\n",
              " 'dave': 102,\n",
              " 'nick': 103,\n",
              " 'jason': 104,\n",
              " 'business': 105,\n",
              " 'sa': 106,\n",
              " 'holdings': 107,\n",
              " 'tim': 108,\n",
              " 'lake': 109,\n",
              " 'river': 110,\n",
              " 'daniel': 111,\n",
              " 'gen': 112,\n",
              " 'ben': 113,\n",
              " 'district': 114,\n",
              " 'analyst': 115,\n",
              " 'technology': 116,\n",
              " 'prnewswire': 117,\n",
              " 'gold': 118,\n",
              " 'george': 119,\n",
              " 'tony': 120,\n",
              " 'w': 121,\n",
              " 'capital': 122,\n",
              " 'f': 123,\n",
              " 'mayor': 124,\n",
              " 'ceo': 125,\n",
              " 'executive': 126,\n",
              " 'on': 127,\n",
              " 'center': 128,\n",
              " 'research': 129,\n",
              " 'plc': 130,\n",
              " 'joseph': 131,\n",
              " 'ag': 132,\n",
              " 'greg': 133,\n",
              " 'ca': 134,\n",
              " 'stephen': 135,\n",
              " 'software': 136,\n",
              " 'judge': 137,\n",
              " 'alex': 138,\n",
              " 'khan': 139,\n",
              " 'county': 140,\n",
              " 'gary': 141,\n",
              " 'k': 142,\n",
              " 'o': 143,\n",
              " 'province': 144,\n",
              " 'adam': 145,\n",
              " 'le': 146,\n",
              " 'ms': 147,\n",
              " 'jean': 148,\n",
              " 'h': 149,\n",
              " 'limited': 150,\n",
              " 'general': 151,\n",
              " 'charles': 152,\n",
              " 'ahmed': 153,\n",
              " 'ed': 154,\n",
              " 'wire': 155,\n",
              " 'valley': 156,\n",
              " 'thomas': 157,\n",
              " 'management': 158,\n",
              " 'mohammed': 159,\n",
              " 'jon': 160,\n",
              " 'martin': 161,\n",
              " 'ryan': 162,\n",
              " 'gov': 163,\n",
              " 'club': 164,\n",
              " 'abu': 165,\n",
              " 'media': 166,\n",
              " 'pro': 167,\n",
              " 'rep': 168,\n",
              " 'lee': 169,\n",
              " 'bin': 170,\n",
              " 'resources': 171,\n",
              " 'actress': 172,\n",
              " 'don': 173,\n",
              " 'venture': 174,\n",
              " 'financial': 175,\n",
              " 'â€¢': 176,\n",
              " 'tv': 177,\n",
              " 'calif': 178,\n",
              " 'home': 179,\n",
              " 'system': 180,\n",
              " 'beach': 181,\n",
              " 'commissioner': 182,\n",
              " 'kim': 183,\n",
              " 'anthony': 184,\n",
              " 'alan': 185,\n",
              " 'rick': 186,\n",
              " 'von': 187,\n",
              " 'justice': 188,\n",
              " 'firstcall': 189,\n",
              " 'marc': 190,\n",
              " 'ron': 191,\n",
              " 'llc': 192,\n",
              " 'solutions': 193,\n",
              " 'manager': 194,\n",
              " 'daily': 195,\n",
              " 'i': 196,\n",
              " 'william': 197,\n",
              " 'mohammad': 198,\n",
              " 'information': 199,\n",
              " 'bank': 200,\n",
              " 'island': 201,\n",
              " 'md': 202,\n",
              " 'senior': 203,\n",
              " 'â‚¬': 204,\n",
              " 'sam': 205,\n",
              " 'city': 206,\n",
              " 'pharmaceuticals': 207,\n",
              " 'steven': 208,\n",
              " 'jonathan': 209,\n",
              " 'jack': 210,\n",
              " 'patrick': 211,\n",
              " 'no': 212,\n",
              " 'san': 213,\n",
              " 'symbol': 214,\n",
              " 'attorney': 215,\n",
              " 'ken': 216,\n",
              " 'defender': 217,\n",
              " 'di': 218,\n",
              " 'association': 219,\n",
              " 'secretary': 220,\n",
              " 'midfielder': 221,\n",
              " 'website': 222,\n",
              " 'larry': 223,\n",
              " 'striker': 224,\n",
              " 'ny': 225,\n",
              " 'singer': 226,\n",
              " 'rob': 227,\n",
              " 'photo': 228,\n",
              " 'va': 229,\n",
              " 'champion': 230,\n",
              " 'communications': 231,\n",
              " 'company': 232,\n",
              " 'josh': 233,\n",
              " 'prime': 234,\n",
              " 'national': 235,\n",
              " 'jr': 236,\n",
              " 'profile': 237,\n",
              " 'project': 238,\n",
              " 'lawyer': 239,\n",
              " 'mary': 240,\n",
              " 'ray': 241,\n",
              " 'professor': 242,\n",
              " 'stadium': 243,\n",
              " 'networks': 244,\n",
              " 'jennifer': 245,\n",
              " 'kumar': 246,\n",
              " 'services': 247,\n",
              " 'amex': 248,\n",
              " 'red': 249,\n",
              " 'please': 250,\n",
              " 'sen': 251,\n",
              " 'mp': 252,\n",
              " 'air': 253,\n",
              " 'andy': 254,\n",
              " 'industries': 255,\n",
              " 'du': 256,\n",
              " 'hospital': 257,\n",
              " 'leader': 258,\n",
              " 'brad': 259,\n",
              " 'doug': 260,\n",
              " 'phil': 261,\n",
              " 'global': 262,\n",
              " 'justin': 263,\n",
              " 'carlos': 264,\n",
              " 'new': 265,\n",
              " 'uk': 266,\n",
              " 'los': 267,\n",
              " 'y': 268,\n",
              " 'governor': 269,\n",
              " 'bay': 270,\n",
              " 'matthew': 271,\n",
              " 'craig': 272,\n",
              " 'jay': 273,\n",
              " 'up': 274,\n",
              " 'jan': 275,\n",
              " 'creek': 276,\n",
              " 'market': 277,\n",
              " 'will': 278,\n",
              " 'guard': 279,\n",
              " 'mass': 280,\n",
              " 'hotel': 281,\n",
              " 'is': 282,\n",
              " 'index': 283,\n",
              " 'be': 284,\n",
              " 'partners': 285,\n",
              " 'lt': 286,\n",
              " 'bruce': 287,\n",
              " 'susan': 288,\n",
              " 'fund': 289,\n",
              " 'you': 290,\n",
              " 'west': 291,\n",
              " 'am': 292,\n",
              " 'network': 293,\n",
              " 'mine': 294,\n",
              " 'high': 295,\n",
              " 'foundation': 296,\n",
              " 'captain': 297,\n",
              " 'alexander': 298,\n",
              " 'township': 299,\n",
              " 'dennis': 300,\n",
              " 'maria': 301,\n",
              " 'todd': 302,\n",
              " 'na': 303,\n",
              " 'medical': 304,\n",
              " 'not': 305,\n",
              " 'church': 306,\n",
              " 'non': 307,\n",
              " 'receiver': 308,\n",
              " 'actor': 309,\n",
              " 'lisa': 310,\n",
              " 'anti': 311,\n",
              " 'ahmad': 312,\n",
              " 'program': 313,\n",
              " 'power': 314,\n",
              " 'sarah': 315,\n",
              " 'ian': 316,\n",
              " 'institute': 317,\n",
              " 'jerry': 318,\n",
              " 'out': 319,\n",
              " 'carl': 320,\n",
              " 'aaron': 321,\n",
              " 'keith': 322,\n",
              " 'email': 323,\n",
              " 'grand': 324,\n",
              " 'fred': 325,\n",
              " 'old': 326,\n",
              " 'school': 327,\n",
              " 'digital': 328,\n",
              " 'christopher': 329,\n",
              " 'roger': 330,\n",
              " 'ill': 331,\n",
              " 'mountain': 332,\n",
              " 'cancer': 333,\n",
              " 'with': 334,\n",
              " 'simon': 335,\n",
              " 'fla': 336,\n",
              " 'star': 337,\n",
              " 'sun': 338,\n",
              " 'radio': 339,\n",
              " 'ii': 340,\n",
              " 'total': 341,\n",
              " 'magazine': 342,\n",
              " 'nj': 343,\n",
              " 'laura': 344,\n",
              " 'jones': 345,\n",
              " 'christian': 346,\n",
              " 'health': 347,\n",
              " 'income': 348,\n",
              " 'jose': 349,\n",
              " 'der': 350,\n",
              " 'gene': 351,\n",
              " \"l'\": 352,\n",
              " 'officer': 353,\n",
              " 'east': 354,\n",
              " 'vice': 355,\n",
              " 'kyle': 356,\n",
              " 'amy': 357,\n",
              " 'des': 358,\n",
              " 'u': 359,\n",
              " 'editor': 360,\n",
              " 'rs': 361,\n",
              " 'sports': 362,\n",
              " 'golf': 363,\n",
              " 'your': 364,\n",
              " 'goalkeeper': 365,\n",
              " 'jeremy': 366,\n",
              " 'jamie': 367,\n",
              " 'online': 368,\n",
              " 'un': 369,\n",
              " 'ga': 370,\n",
              " 'act': 371,\n",
              " 'mail': 372,\n",
              " 'society': 373,\n",
              " 'joel': 374,\n",
              " 'hill': 375,\n",
              " 'funeral': 376,\n",
              " 'blue': 377,\n",
              " 'service': 378,\n",
              " 'party': 379,\n",
              " 'gas': 380,\n",
              " 'force': 381,\n",
              " 'super': 382,\n",
              " 'an': 383,\n",
              " 'louis': 384,\n",
              " 'karen': 385,\n",
              " 'col': 386,\n",
              " 'drive': 387,\n",
              " 'mount': 388,\n",
              " 'pat': 389,\n",
              " 'danny': 390,\n",
              " 'max': 391,\n",
              " 'smith': 392,\n",
              " 'street': 393,\n",
              " 'usa': 394,\n",
              " 'anne': 395,\n",
              " 'theatre': 396,\n",
              " 'press': 397,\n",
              " 'village': 398,\n",
              " 'pierre': 399,\n",
              " 'brown': 400,\n",
              " 'terry': 401,\n",
              " 'data': 402,\n",
              " 'festival': 403,\n",
              " 'art': 404,\n",
              " 'disease': 405,\n",
              " 'en': 406,\n",
              " 'germany': 407,\n",
              " 'mo': 408,\n",
              " 'linda': 409,\n",
              " 'pete': 410,\n",
              " 'contact': 411,\n",
              " 'king': 412,\n",
              " 'mohamed': 413,\n",
              " 'mobile': 414,\n",
              " 'senator': 415,\n",
              " 'steel': 416,\n",
              " 'trust': 417,\n",
              " 'ann': 418,\n",
              " 'randy': 419,\n",
              " 'wife': 420,\n",
              " 'college': 421,\n",
              " 'sean': 422,\n",
              " 'north': 423,\n",
              " 'racing': 424,\n",
              " 'jeffrey': 425,\n",
              " 'from': 426,\n",
              " 'les': 427,\n",
              " 'philip': 428,\n",
              " 'reuters': 429,\n",
              " 'wireless': 430,\n",
              " 'linebacker': 431,\n",
              " 'hall': 432,\n",
              " 'port': 433,\n",
              " 'anna': 434,\n",
              " 'black': 435,\n",
              " 'mining': 436,\n",
              " 'chuck': 437,\n",
              " 'team': 438,\n",
              " 'barbara': 439,\n",
              " 'jessica': 440,\n",
              " 'ted': 441,\n",
              " '1': 442,\n",
              " 'drug': 443,\n",
              " 'station': 444,\n",
              " 'shah': 445,\n",
              " 'town': 446,\n",
              " 'as': 447,\n",
              " 'or': 448,\n",
              " 'nc': 449,\n",
              " 'rich': 450,\n",
              " 'defenseman': 451,\n",
              " 'investor': 452,\n",
              " 'barry': 453,\n",
              " 'camp': 454,\n",
              " 'world': 455,\n",
              " 'announces': 456,\n",
              " 'wayne': 457,\n",
              " 'report': 458,\n",
              " 'michelle': 459,\n",
              " 'development': 460,\n",
              " 'chicken': 461,\n",
              " 'edward': 462,\n",
              " 'consolidated': 463,\n",
              " 'quarterback': 464,\n",
              " 'victor': 465,\n",
              " 'cell': 466,\n",
              " 'board': 467,\n",
              " 'via': 468,\n",
              " 'nancy': 469,\n",
              " 'silver': 470,\n",
              " 'express': 471,\n",
              " 'man': 472,\n",
              " 'about': 473,\n",
              " 'muhammad': 474,\n",
              " 'hills': 475,\n",
              " 'chart': 476,\n",
              " 'da': 477,\n",
              " 'equity': 478,\n",
              " 'free': 479,\n",
              " 'andrea': 480,\n",
              " 'big': 481,\n",
              " 'williams': 482,\n",
              " 'resort': 483,\n",
              " 'asset': 484,\n",
              " 'oil': 485,\n",
              " 'newspaper': 486,\n",
              " 'li': 487,\n",
              " 'state': 488,\n",
              " 'jane': 489,\n",
              " 'reporter': 490,\n",
              " 'south': 491,\n",
              " 'white': 492,\n",
              " 'journal': 493,\n",
              " 'julie': 494,\n",
              " 'luis': 495,\n",
              " 'neil': 496,\n",
              " 'products': 497,\n",
              " 'lane': 498,\n",
              " 'roy': 499,\n",
              " 'stakes': 500,\n",
              " 'unaudited': 501,\n",
              " 'head': 502,\n",
              " 'henry': 503,\n",
              " 'fm': 504,\n",
              " 'point': 505,\n",
              " 'dean': 506,\n",
              " 'mario': 507,\n",
              " 'field': 508,\n",
              " 'are': 509,\n",
              " 'real': 510,\n",
              " 'securities': 511,\n",
              " 'solar': 512,\n",
              " 'green': 513,\n",
              " 'china': 514,\n",
              " 'miss': 515,\n",
              " 'juan': 516,\n",
              " 'day': 517,\n",
              " 'channel': 518,\n",
              " 'bobby': 519,\n",
              " 'au': 520,\n",
              " 'harry': 521,\n",
              " 'university': 522,\n",
              " 'mich': 523,\n",
              " 'elizabeth': 524,\n",
              " 'johnson': 525,\n",
              " 'water': 526,\n",
              " 'investment': 527,\n",
              " 'acid': 528,\n",
              " 'us': 529,\n",
              " 'times': 530,\n",
              " 'property': 531,\n",
              " 'agent': 532,\n",
              " 'museum': 533,\n",
              " 'hp': 534,\n",
              " 'court': 535,\n",
              " 'brett': 536,\n",
              " 'award': 537,\n",
              " 'awards': 538,\n",
              " 'entertainment': 539,\n",
              " 'long': 540,\n",
              " 'antonio': 541,\n",
              " 'sheikh': 542,\n",
              " 'holding': 543,\n",
              " 'hassan': 544,\n",
              " 'rachel': 545,\n",
              " 'mm': 546,\n",
              " 'first': 547,\n",
              " 'iron': 548,\n",
              " 'jimmy': 549,\n",
              " 'reliever': 550,\n",
              " 'per': 551,\n",
              " 'pink': 552,\n",
              " 'fort': 553,\n",
              " 'charlie': 554,\n",
              " 'authority': 555,\n",
              " 'kate': 556,\n",
              " 'rhp': 557,\n",
              " 'ram': 558,\n",
              " 'erik': 559,\n",
              " 'deputy': 560,\n",
              " 'marie': 561,\n",
              " 'ralph': 562,\n",
              " 'annual': 563,\n",
              " 'centre': 564,\n",
              " 'jake': 565,\n",
              " 'stock': 566,\n",
              " 'ibrahim': 567,\n",
              " 'kelly': 568,\n",
              " 'council': 569,\n",
              " 'hot': 570,\n",
              " 'bryan': 571,\n",
              " 'ap': 572,\n",
              " 'airport': 573,\n",
              " 'il': 574,\n",
              " 'union': 575,\n",
              " 'sc': 576,\n",
              " 'th': 577,\n",
              " 'wide': 578,\n",
              " 'chad': 579,\n",
              " 'dick': 580,\n",
              " 'american': 581,\n",
              " 'mother': 582,\n",
              " 'derek': 583,\n",
              " 'z': 584,\n",
              " 'marketwire': 585,\n",
              " 'ice': 586,\n",
              " 'france': 587,\n",
              " 'fire': 588,\n",
              " 'therapy': 589,\n",
              " 'file': 590,\n",
              " 'kurt': 591,\n",
              " 'bridge': 592,\n",
              " 'prosecutor': 593,\n",
              " 'platform': 594,\n",
              " 'iii': 595,\n",
              " 'nicole': 596,\n",
              " 'this': 597,\n",
              " 'pitcher': 598,\n",
              " 'nathan': 599,\n",
              " 'middle': 600,\n",
              " 'enterprise': 601,\n",
              " 'bloomberg': 602,\n",
              " 'datuk': 603,\n",
              " 'self': 604,\n",
              " 'special': 605,\n",
              " 'estate': 606,\n",
              " 'springs': 607,\n",
              " 'stephanie': 608,\n",
              " 'at': 609,\n",
              " 'arts': 610,\n",
              " 'tyler': 611,\n",
              " 'server': 612,\n",
              " 'winger': 613,\n",
              " 're': 614,\n",
              " 'capt': 615,\n",
              " 'ore': 616,\n",
              " 'that': 617,\n",
              " 'care': 618,\n",
              " 'region': 619,\n",
              " 'nicholas': 620,\n",
              " 'agency': 621,\n",
              " 'copper': 622,\n",
              " 'best': 623,\n",
              " 'industry': 624,\n",
              " 'starring': 625,\n",
              " 'credit': 626,\n",
              " 'brandon': 627,\n",
              " 'ng': 628,\n",
              " 'assets': 629,\n",
              " 'community': 630,\n",
              " 'golden': 631,\n",
              " 'walter': 632,\n",
              " 'katie': 633,\n",
              " 'cash': 634,\n",
              " 'hitter': 635,\n",
              " '2': 636,\n",
              " 'bernard': 637,\n",
              " 'santa': 638,\n",
              " 'ma': 639,\n",
              " 'wash': 640,\n",
              " 'security': 641,\n",
              " 'taylor': 642,\n",
              " 'life': 643,\n",
              " 'luke': 644,\n",
              " 'andre': 645,\n",
              " 'series': 646,\n",
              " 'guy': 647,\n",
              " 'las': 648,\n",
              " 'Ã¢': 649,\n",
              " 'correspondent': 650,\n",
              " 'great': 651,\n",
              " 'elementary': 652,\n",
              " 'abdel': 653,\n",
              " 'therapeutics': 654,\n",
              " 'ventures': 655,\n",
              " 'foods': 656,\n",
              " 'prof': 657,\n",
              " 'all': 658,\n",
              " 'vs': 659,\n",
              " 'ab': 660,\n",
              " 'tax': 661,\n",
              " 'bn': 662,\n",
              " 'down': 663,\n",
              " 'avenue': 664,\n",
              " 'healthcare': 665,\n",
              " 'mrs': 666,\n",
              " 'ambassador': 667,\n",
              " 'sharma': 668,\n",
              " 'wang': 669,\n",
              " 'division': 670,\n",
              " 'drew': 671,\n",
              " 'brothers': 672,\n",
              " 'hd': 673,\n",
              " 'do': 674,\n",
              " 'producer': 675,\n",
              " 'top': 676,\n",
              " 'live': 677,\n",
              " 'glenn': 678,\n",
              " 'lauren': 679,\n",
              " 'ho': 680,\n",
              " 'newswire': 681,\n",
              " 'engine': 682,\n",
              " 'emily': 683,\n",
              " 'maj': 684,\n",
              " 'viagra': 685,\n",
              " 'minn': 686,\n",
              " 'cup': 687,\n",
              " 'ky': 688,\n",
              " 'mi': 689,\n",
              " 'texas': 690,\n",
              " 'te': 691,\n",
              " 'darren': 692,\n",
              " 'robin': 693,\n",
              " 'electric': 694,\n",
              " 'ex': 695,\n",
              " 'sri': 696,\n",
              " 'subsidiary': 697,\n",
              " 'bishop': 698,\n",
              " 'q': 699,\n",
              " 'ind': 700,\n",
              " 'pm': 701,\n",
              " 'insurance': 702,\n",
              " 'gallery': 703,\n",
              " 'cornerback': 704,\n",
              " 'karl': 705,\n",
              " 'managing': 706,\n",
              " 'me': 707,\n",
              " 'howard': 708,\n",
              " 'royal': 709,\n",
              " 'dale': 710,\n",
              " 'advanced': 711,\n",
              " 'shane': 712,\n",
              " 'jacques': 713,\n",
              " 'time': 714,\n",
              " 'off': 715,\n",
              " 'plant': 716,\n",
              " 'unit': 717,\n",
              " 'coal': 718,\n",
              " 'carol': 719,\n",
              " 'kenneth': 720,\n",
              " 'marco': 721,\n",
              " 'blood': 722,\n",
              " 'lady': 723,\n",
              " 'vincent': 724,\n",
              " 'christine': 725,\n",
              " 'adrian': 726,\n",
              " 'vladimir': 727,\n",
              " 'globe': 728,\n",
              " 'stuart': 729,\n",
              " 'liabilities': 730,\n",
              " 'johnny': 731,\n",
              " 'michel': 732,\n",
              " 'post': 733,\n",
              " 'sea': 734,\n",
              " 'honda': 735,\n",
              " 'economist': 736,\n",
              " 'control': 737,\n",
              " 'area': 738,\n",
              " 'law': 739,\n",
              " 'jordan': 740,\n",
              " 'marcus': 741,\n",
              " 'operation': 742,\n",
              " 'superintendent': 743,\n",
              " 'one': 744,\n",
              " 'jorge': 745,\n",
              " 'boss': 746,\n",
              " 'processor': 747,\n",
              " 'call': 748,\n",
              " 'starter': 749,\n",
              " 'billy': 750,\n",
              " 'ivan': 751,\n",
              " 'little': 752,\n",
              " 'prince': 753,\n",
              " 'cells': 754,\n",
              " 'wis': 755,\n",
              " 'beth': 756,\n",
              " 'human': 757,\n",
              " 'arthur': 758,\n",
              " 'donald': 759,\n",
              " 'speaker': 760,\n",
              " 'dam': 761,\n",
              " 'contributed': 762,\n",
              " 'tree': 763,\n",
              " 'hans': 764,\n",
              " 'low': 765,\n",
              " 'class': 766,\n",
              " 'falls': 767,\n",
              " 'my': 768,\n",
              " 'public': 769,\n",
              " 'band': 770,\n",
              " 'kathy': 771,\n",
              " 'diane': 772,\n",
              " 'incorporated': 773,\n",
              " 'plus': 774,\n",
              " 'allen': 775,\n",
              " 'family': 776,\n",
              " 'premier': 777,\n",
              " 'saint': 778,\n",
              " 'young': 779,\n",
              " 'joint': 780,\n",
              " 'show': 781,\n",
              " 'sir': 782,\n",
              " 'islands': 783,\n",
              " 'reports': 784,\n",
              " 'ashley': 785,\n",
              " 'optical': 786,\n",
              " 'spa': 787,\n",
              " 'hussain': 788,\n",
              " 'music': 789,\n",
              " 'core': 790,\n",
              " 'albert': 791,\n",
              " 'rebecca': 792,\n",
              " 'exchange': 793,\n",
              " 'earnings': 794,\n",
              " 'brent': 795,\n",
              " 'safety': 796,\n",
              " 'omar': 797,\n",
              " 'today': 798,\n",
              " 'league': 799,\n",
              " 'lawrence': 800,\n",
              " 'designer': 801,\n",
              " 'central': 802,\n",
              " 'rock': 803,\n",
              " 'code': 804,\n",
              " 'lord': 805,\n",
              " 'marine': 806,\n",
              " 'colin': 807,\n",
              " 'properties': 808,\n",
              " 'amanda': 809,\n",
              " 'ul': 810,\n",
              " 'samuel': 811,\n",
              " 'patricia': 812,\n",
              " 'ross': 813,\n",
              " 'patel': 814,\n",
              " 'against': 815,\n",
              " 'lb': 816,\n",
              " 'syed': 817,\n",
              " 'loss': 818,\n",
              " 'owner': 819,\n",
              " 'melissa': 820,\n",
              " 'nguyen': 821,\n",
              " 'syndrome': 822,\n",
              " 'miguel': 823,\n",
              " 'id': 824,\n",
              " 'ford': 825,\n",
              " 'lynn': 826,\n",
              " 'bar': 827,\n",
              " 'alliance': 828,\n",
              " 'markets': 829,\n",
              " 'eddie': 830,\n",
              " 'roman': 831,\n",
              " 'ala': 832,\n",
              " 'temple': 833,\n",
              " 'theater': 834,\n",
              " 'rd': 835,\n",
              " 'tan': 836,\n",
              " 'basin': 837,\n",
              " 'house': 838,\n",
              " 'benjamin': 839,\n",
              " 'country': 840,\n",
              " 'worldwide': 841,\n",
              " 'natural': 842,\n",
              " 'square': 843,\n",
              " 'mohd': 844,\n",
              " 'video': 845,\n",
              " 'author': 846,\n",
              " 'car': 847,\n",
              " 'story': 848,\n",
              " 'otc': 849,\n",
              " 'rookie': 850,\n",
              " 'sauce': 851,\n",
              " 'seth': 852,\n",
              " 'baby': 853,\n",
              " 'cream': 854,\n",
              " 'regional': 855,\n",
              " 'metal': 856,\n",
              " 'francis': 857,\n",
              " 'travis': 858,\n",
              " 'shawn': 859,\n",
              " 'laser': 860,\n",
              " 'sara': 861,\n",
              " 'joey': 862,\n",
              " 'vaccine': 863,\n",
              " 'investments': 864,\n",
              " 'tommy': 865,\n",
              " 'ronald': 866,\n",
              " 'rahman': 867,\n",
              " 'fernando': 868,\n",
              " 'commander': 869,\n",
              " 'zhang': 870,\n",
              " 'education': 871,\n",
              " 'multi': 872,\n",
              " 'cheese': 873,\n",
              " 'telecom': 874,\n",
              " 'que': 875,\n",
              " 'japan': 876,\n",
              " 'douglas': 877,\n",
              " 'ip': 878,\n",
              " 'goalie': 879,\n",
              " 'gerald': 880,\n",
              " 'tenn': 881,\n",
              " 'fl': 882,\n",
              " 'rate': 883,\n",
              " 'agreement': 884,\n",
              " 'lou': 885,\n",
              " 'conn': 886,\n",
              " 'conference': 887,\n",
              " 'manuel': 888,\n",
              " 'base': 889,\n",
              " 'light': 890,\n",
              " 'jackson': 891,\n",
              " 'russell': 892,\n",
              " 'records': 893,\n",
              " 'auto': 894,\n",
              " 'jo': 895,\n",
              " 'short': 896,\n",
              " 'italy': 897,\n",
              " 'jacob': 898,\n",
              " 'model': 899,\n",
              " 'wind': 900,\n",
              " 'founder': 901,\n",
              " 'morgan': 902,\n",
              " 'alpha': 903,\n",
              " 'pinksheets': 904,\n",
              " 'ohio': 905,\n",
              " 'gordon': 906,\n",
              " 'roberto': 907,\n",
              " 'geoff': 908,\n",
              " 'police': 909,\n",
              " 'zone': 910,\n",
              " 'warren': 911,\n",
              " 'electronics': 912,\n",
              " 'laboratories': 913,\n",
              " 'llp': 914,\n",
              " 'rao': 915,\n",
              " 'member': 916,\n",
              " 'davis': 917,\n",
              " 'web': 918,\n",
              " 'petroleum': 919,\n",
              " 'father': 920,\n",
              " 'wood': 921,\n",
              " 'we': 922,\n",
              " 'committee': 923,\n",
              " 'grant': 924,\n",
              " 'catherine': 925,\n",
              " 'jill': 926,\n",
              " 'pharma': 927,\n",
              " 'course': 928,\n",
              " 'sister': 929,\n",
              " 'card': 930,\n",
              " 'upper': 931,\n",
              " 'keeper': 932,\n",
              " 'rabbi': 933,\n",
              " '3': 934,\n",
              " 'local': 935,\n",
              " 'carbon': 936,\n",
              " 'lp': 937,\n",
              " 'into': 938,\n",
              " 'hotels': 939,\n",
              " 'ski': 940,\n",
              " 'inch': 941,\n",
              " 'et': 942,\n",
              " 'erin': 943,\n",
              " 'other': 944,\n",
              " 'share': 945,\n",
              " 'minerals': 946,\n",
              " 'ellen': 947,\n",
              " 'se': 948,\n",
              " 'open': 949,\n",
              " 'heather': 950,\n",
              " 'edu': 951,\n",
              " 'sub': 952,\n",
              " 'certified': 953,\n",
              " 'mall': 954,\n",
              " 'pinch': 955,\n",
              " 'clinical': 956,\n",
              " 'forest': 957,\n",
              " 'francois': 958,\n",
              " 'support': 959,\n",
              " 'boulevard': 960,\n",
              " 'heights': 961,\n",
              " 'diamond': 962,\n",
              " 'outfielder': 963,\n",
              " 'tel': 964,\n",
              " 'games': 965,\n",
              " 'francisco': 966,\n",
              " 'sony': 967,\n",
              " 'jesse': 968,\n",
              " 'ultra': 969,\n",
              " 'evan': 970,\n",
              " 'term': 971,\n",
              " 'protein': 972,\n",
              " 'kg': 973,\n",
              " 'joan': 974,\n",
              " 'official': 975,\n",
              " 'subsidiaries': 976,\n",
              " 'tribe': 977,\n",
              " 'spokesperson': 978,\n",
              " 'enterprises': 979,\n",
              " 'christina': 980,\n",
              " 'go': 981,\n",
              " 'homes': 982,\n",
              " 'affairs': 983,\n",
              " 'motor': 984,\n",
              " 'thomson': 985,\n",
              " 'zach': 986,\n",
              " 'sergei': 987,\n",
              " 'film': 988,\n",
              " 'child': 989,\n",
              " 'surgery': 990,\n",
              " 'casino': 991,\n",
              " 'trevor': 992,\n",
              " 'silicon': 993,\n",
              " 'colo': 994,\n",
              " 'pass': 995,\n",
              " 'mc': 996,\n",
              " 'mortgage': 997,\n",
              " 'oral': 998,\n",
              " 'palace': 999,\n",
              " 'donna': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJSClCnowW7O"
      },
      "source": [
        "#_________________________________________________MAKING THE EMBEEDING MATRIX_______________________________________\n",
        "\n",
        "embedding_matrix = numpy.zeros(( size_of_vocabulary_dataset , 300))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrMGPk57wW7O"
      },
      "source": [
        "#____________________________________________________________________WORD EMBEEDING MATRIX PREPARED_____________________________________________\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    #print(i,word)\n",
        "    if word in vocab_word2vec :\n",
        "        #Columns Vice Vectorize i.e. for Each Vocab Word\n",
        "        embedding_matrix[i] =  word2vec[word]\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n12QTE4NsUUU"
      },
      "source": [
        "#**DELETING THE WORDTOVEC MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9B0DJ05GB8x",
        "outputId": "6b7a4ee6-1fd2-467f-8653-04d1ace09602"
      },
      "source": [
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "#_____________________REMOVING THE WORDTOVEC MODEL AS NO REFER FRTHER AFTER EMBEEDING MATRIX_________________________\n",
        "\n",
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "del word2vec\n",
        "\n",
        "\n",
        "#__________________________TESTING___________________________________\n",
        "embedding_matrix[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.26171875e-01,  9.32617188e-02, -2.08740234e-02,  3.29589844e-02,\n",
              "       -3.78417969e-02, -1.96289062e-01, -8.34960938e-02, -9.32617188e-02,\n",
              "        2.23632812e-01,  6.88476562e-02, -2.96020508e-03, -2.46093750e-01,\n",
              "       -3.90625000e-02,  6.10351562e-02, -8.69140625e-02,  3.65234375e-01,\n",
              "        1.66015625e-01,  1.74804688e-01, -1.36718750e-01, -1.24511719e-02,\n",
              "       -2.30468750e-01, -9.71679688e-02, -9.47265625e-02,  9.22851562e-02,\n",
              "       -3.45703125e-01, -8.85009766e-03, -1.02539062e-01,  1.55273438e-01,\n",
              "       -4.15039062e-02, -2.47070312e-01, -7.95898438e-02,  2.04101562e-01,\n",
              "       -2.07031250e-01, -2.96875000e-01, -1.15356445e-02, -1.28173828e-02,\n",
              "       -1.21582031e-01,  1.52343750e-01,  4.12597656e-02, -3.71093750e-02,\n",
              "       -3.53515625e-01, -8.05664062e-02,  3.56445312e-02,  3.84765625e-01,\n",
              "        5.12695312e-02, -8.78906250e-02, -1.56250000e-01,  2.29492188e-01,\n",
              "       -2.44140625e-02,  1.46484375e-01, -3.10546875e-01,  3.14453125e-01,\n",
              "       -1.02539062e-01,  5.39062500e-01,  9.46044922e-03,  3.46679688e-02,\n",
              "       -2.34375000e-01, -9.37500000e-02,  1.23901367e-02,  7.91015625e-02,\n",
              "       -8.39843750e-02, -1.34765625e-01, -2.96875000e-01,  7.28607178e-04,\n",
              "       -3.08593750e-01, -4.39453125e-02, -2.31445312e-01, -6.56127930e-04,\n",
              "        2.87109375e-01,  2.27539062e-01,  2.55859375e-01,  1.78710938e-01,\n",
              "        3.63769531e-02, -9.52148438e-03,  2.89062500e-01,  6.00585938e-02,\n",
              "        1.04980469e-01, -8.85009766e-04, -6.83593750e-02,  7.12890625e-02,\n",
              "       -1.88476562e-01, -4.10156250e-02, -3.54003906e-02, -1.15234375e-01,\n",
              "        2.33398438e-01,  9.82666016e-03,  3.34472656e-02,  1.30859375e-01,\n",
              "        2.13867188e-01,  1.80664062e-02, -2.22778320e-03,  4.29687500e-02,\n",
              "       -1.43554688e-01, -4.88281250e-01,  1.31835938e-01,  4.45312500e-01,\n",
              "       -8.49609375e-02, -1.92382812e-01, -1.77001953e-03, -2.20947266e-02,\n",
              "        2.47192383e-03,  9.86328125e-02,  3.17382812e-02,  4.00390625e-02,\n",
              "       -1.20117188e-01,  2.09960938e-02, -2.87109375e-01,  1.74804688e-01,\n",
              "        1.63085938e-01,  1.55273438e-01, -2.55859375e-01, -5.02929688e-02,\n",
              "        1.42211914e-02, -9.71679688e-02,  1.69921875e-01, -1.90429688e-02,\n",
              "        2.19726562e-03, -1.21582031e-01,  1.58203125e-01,  3.71093750e-02,\n",
              "        9.47265625e-02, -9.03320312e-03,  4.88281250e-02,  9.37500000e-02,\n",
              "        1.34765625e-01, -1.99218750e-01, -1.48437500e-01,  1.52343750e-01,\n",
              "        1.17187500e-01,  4.24804688e-02, -4.17480469e-02,  4.10156250e-02,\n",
              "       -1.53320312e-01,  2.98828125e-01, -2.38037109e-02,  9.61914062e-02,\n",
              "       -2.14843750e-01, -3.36914062e-02, -4.44335938e-02, -1.05468750e-01,\n",
              "        4.70703125e-01, -5.19531250e-01, -6.68945312e-02,  8.10546875e-02,\n",
              "        1.81640625e-01, -1.12792969e-01,  1.07910156e-01, -4.27246094e-02,\n",
              "       -2.69531250e-01,  2.85156250e-01,  2.96875000e-01, -2.77343750e-01,\n",
              "       -2.10937500e-01, -1.26953125e-01, -1.25000000e-01, -2.47070312e-01,\n",
              "        2.73437500e-01,  2.98828125e-01, -4.10156250e-02, -1.67083740e-03,\n",
              "        1.19140625e-01,  1.82617188e-01, -1.41601562e-01, -3.54003906e-02,\n",
              "        3.28125000e-01, -8.25195312e-02, -3.06396484e-02,  1.27929688e-01,\n",
              "       -3.43750000e-01,  1.35742188e-01, -1.26953125e-01,  5.34667969e-02,\n",
              "        7.81250000e-02, -1.35742188e-01, -5.10253906e-02,  1.85546875e-01,\n",
              "        1.54296875e-01, -2.18750000e-01, -6.88476562e-02, -1.50756836e-02,\n",
              "       -1.10839844e-01, -1.00585938e-01, -4.95605469e-02,  1.95312500e-01,\n",
              "        2.78320312e-02, -7.12890625e-02, -4.39453125e-01,  3.30078125e-01,\n",
              "        1.77734375e-01, -3.84521484e-03,  2.53906250e-01,  1.12304688e-01,\n",
              "       -3.69140625e-01,  5.49316406e-02,  1.51367188e-01,  1.91406250e-01,\n",
              "        1.68457031e-02, -4.78515625e-02, -5.70678711e-03, -2.50000000e-01,\n",
              "       -2.69531250e-01,  3.36914062e-02, -9.66796875e-02,  8.44726562e-02,\n",
              "        2.21679688e-01, -2.14843750e-01,  6.39648438e-02,  4.39453125e-02,\n",
              "       -1.51367188e-01,  4.51171875e-01,  2.38281250e-01,  3.12500000e-02,\n",
              "       -2.92968750e-01,  6.98242188e-02,  1.24511719e-02,  2.26562500e-01,\n",
              "        4.27246094e-02, -1.12304688e-01, -2.65625000e-01,  6.68334961e-03,\n",
              "        1.88476562e-01,  2.46093750e-01,  1.86157227e-03,  2.65625000e-01,\n",
              "        1.25976562e-01, -4.56542969e-02,  1.35742188e-01, -6.88476562e-02,\n",
              "       -1.33789062e-01, -4.27246094e-02,  6.03027344e-02, -3.53515625e-01,\n",
              "       -2.30712891e-02, -2.35351562e-01,  1.55273438e-01,  1.55273438e-01,\n",
              "        2.26562500e-01,  2.61718750e-01, -7.03125000e-02,  2.08984375e-01,\n",
              "        1.07910156e-01, -1.78710938e-01, -8.74023438e-02, -1.12792969e-01,\n",
              "       -4.24804688e-02,  1.94335938e-01, -8.72802734e-03,  1.87500000e-01,\n",
              "       -1.64062500e-01, -1.95312500e-01,  1.41601562e-01,  7.08007812e-02,\n",
              "       -8.39843750e-02,  2.18505859e-02,  8.00781250e-02, -1.27929688e-01,\n",
              "       -1.16210938e-01, -1.25976562e-01,  7.71484375e-02, -2.41699219e-02,\n",
              "        5.14984131e-04,  1.41601562e-01,  2.33398438e-01, -9.76562500e-02,\n",
              "       -1.36718750e-01,  3.80859375e-02, -2.20703125e-01, -8.44726562e-02,\n",
              "        1.29394531e-02, -3.49044800e-04, -1.64062500e-01,  9.13085938e-02,\n",
              "       -8.30078125e-02, -1.55273438e-01,  7.47070312e-02, -4.56542969e-02,\n",
              "       -4.83398438e-02, -3.20312500e-01, -1.39648438e-01,  8.39843750e-02,\n",
              "       -2.08007812e-01,  1.34765625e-01,  2.51770020e-03,  9.47265625e-02,\n",
              "        1.25976562e-01,  3.83300781e-02, -2.55859375e-01,  9.47265625e-02,\n",
              "       -1.43554688e-01,  5.59082031e-02,  2.65625000e-01,  4.27246094e-02,\n",
              "        1.31225586e-02,  6.93359375e-02,  9.08203125e-02, -6.44531250e-02,\n",
              "       -4.82177734e-03,  8.20312500e-02, -2.50000000e-01,  2.67578125e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3PmZSdC1JZc"
      },
      "source": [
        "#**FINAL TRAINING DATA SHAPING AND SEQUENCE GENERATION:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owtYWJe3wW7P"
      },
      "source": [
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "#___________________________________________________________WORD SEQ GENERATIONS__________________________________________________\n",
        "\n",
        "\n",
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "# fit_on_texts----------------> Takes The Text From Corp And Assigns Indexes\n",
        "# texts_to_sequences----------> Generates Heres Indexes Sequences CSV\n",
        "#tokenizer.word_index-----------------> Gives The word Indexing In dictionary Form obn the basis of fit data\n",
        "\n",
        "x_train_sq = tokenizer.texts_to_sequences( x_train )\n",
        "x_val_sq = tokenizer.texts_to_sequences( x_val )\n",
        "x_test_sq = tokenizer.texts_to_sequences( x_test )\n",
        "\n",
        "# saving  Tokenizer\n",
        "with open('/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Unzipped/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "del tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psut47-IwW7P"
      },
      "source": [
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "#_________________________________________GET THE MAX LENGTH OF THE TET______________________________________________________________\n",
        "\n",
        "#\n",
        "########################################################################################################################\n",
        "\n",
        "\n",
        "all_train_test_val = []\n",
        "for i in range( len(x_train_sq) ) :\n",
        "    all_train_test_val.append( len(x_train_sq[i]) )\n",
        "for i in range( len(x_val_sq) ) :\n",
        "    all_train_test_val.append( len(x_val_sq[i]) )\n",
        "for i in range( len(x_test_sq) ) :\n",
        "    all_train_test_val.append( len(x_test_sq[i]) )\n",
        "maximum = max(all_train_test_val)\n",
        "avgLens=sum(all_train_test_val)/len(all_train_test_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rF5a5giwW7P",
        "outputId": "555ba050-5b58-4367-8e79-3c4b88f49408"
      },
      "source": [
        "avgLens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25.09719981015662"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAPRgJ57wW7P"
      },
      "source": [
        "# [frq_train_n,frq_train_p,frq_train_t] = visualize(x_train_sq, y_train )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7lf6xWuwW7Q"
      },
      "source": [
        "# [frq_train_n,frq_train_p,frq_train_t] = visualize(x_val_sq, y_val )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwvfqXcowW7Q"
      },
      "source": [
        "# [frq_train_n,frq_train_p,frq_train_t] = visualize(x_test_sq, y_test )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mdo0DFDDFknx"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Output: [0, 0, 1, 0, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G3GdllcwW7R"
      },
      "source": [
        "#_______________________________________________CONVERT Y TO FLOAT_______________________________________________________\n",
        "\n",
        "for i in range(len(y_train)) :\n",
        "    y_train[i] = float(y_train[i])\n",
        "for i in range(len(y_val)) :\n",
        "    y_val[i] = float(y_val[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ogzNs-RwW7R"
      },
      "source": [
        "#______________________________________________FILTERING THE X ON THRESHOLD VAL_____________________________________________\n",
        "\n",
        "#pad_sequences-----------> Transforms The Data To the 2d Array Of Same dimensions By padding the 0's if short and if fll i.e. 1000 paste as it is\n",
        "threshold = 50   #1000  of the greatest length heres\n",
        "x_train_sq_pd = pad_sequences( x_train_sq , maxlen = threshold )\n",
        "x_val_sq_pd = pad_sequences( x_val_sq , maxlen = threshold )\n",
        "x_test_sq_pd = pad_sequences( x_test_sq , maxlen = threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DQKqLajwW7R"
      },
      "source": [
        "# shallow vanila RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB85Yoa0wW7R",
        "outputId": "41fb6952-1dae-45a9-d5cc-658c7eb02c33"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(size_of_vocabulary_dataset,300,weights=[embedding_matrix],input_length= threshold ,trainable=False)) \n",
        "model.add( SimpleRNN ( 64 , activation = 'sigmoid' ) )\n",
        "model.add( Dense( 1 , activation = 'sigmoid' ) )\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'] )\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('/content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 300)           231949500 \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 64)                23360     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 231,972,925\n",
            "Trainable params: 23,425\n",
            "Non-trainable params: 231,949,500\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B-gVXfXwW7S",
        "outputId": "b2656669-3065-40e0-fac8-fd102fc7e241"
      },
      "source": [
        "history = model.fit(numpy.array(x_train_sq_pd),numpy.array(y_train),batch_size=128,epochs=10,validation_data=(numpy.array(x_val_sq_pd),numpy.array(y_val)),verbose=1,callbacks=[es,mc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.6755 - acc: 0.6009\n",
            "Epoch 00001: val_acc improved from -inf to 0.64766, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "51/51 [==============================] - 11s 211ms/step - loss: 0.6755 - acc: 0.6009 - val_loss: 0.6369 - val_acc: 0.6477\n",
            "Epoch 2/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.6164 - acc: 0.6721\n",
            "Epoch 00002: val_acc improved from 0.64766 to 0.69533, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "51/51 [==============================] - 15s 302ms/step - loss: 0.6164 - acc: 0.6721 - val_loss: 0.6055 - val_acc: 0.6953\n",
            "Epoch 3/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.5826 - acc: 0.7084\n",
            "Epoch 00003: val_acc improved from 0.69533 to 0.71028, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "51/51 [==============================] - 23s 450ms/step - loss: 0.5826 - acc: 0.7084 - val_loss: 0.5733 - val_acc: 0.7103\n",
            "Epoch 4/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.5383 - acc: 0.7452\n",
            "Epoch 00004: val_acc improved from 0.71028 to 0.75280, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "51/51 [==============================] - 21s 414ms/step - loss: 0.5383 - acc: 0.7452 - val_loss: 0.5167 - val_acc: 0.7528\n",
            "Epoch 5/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.4192 - acc: 0.8131\n",
            "Epoch 00005: val_acc improved from 0.75280 to 0.85701, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "51/51 [==============================] - 26s 508ms/step - loss: 0.4192 - acc: 0.8131 - val_loss: 0.3514 - val_acc: 0.8570\n",
            "Epoch 6/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.3262 - acc: 0.8650\n",
            "Epoch 00006: val_acc improved from 0.85701 to 0.87243, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "51/51 [==============================] - 23s 452ms/step - loss: 0.3262 - acc: 0.8650 - val_loss: 0.3326 - val_acc: 0.8724\n",
            "Epoch 7/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.3069 - acc: 0.8749\n",
            "Epoch 00007: val_acc improved from 0.87243 to 0.87383, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "51/51 [==============================] - 24s 474ms/step - loss: 0.3069 - acc: 0.8749 - val_loss: 0.3185 - val_acc: 0.8738\n",
            "Epoch 8/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.3117 - acc: 0.8707\n",
            "Epoch 00008: val_acc did not improve from 0.87383\n",
            "51/51 [==============================] - 4s 84ms/step - loss: 0.3117 - acc: 0.8707 - val_loss: 0.3635 - val_acc: 0.8467\n",
            "Epoch 9/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.2980 - acc: 0.8791\n",
            "Epoch 00009: val_acc improved from 0.87383 to 0.87944, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "51/51 [==============================] - 19s 372ms/step - loss: 0.2980 - acc: 0.8791 - val_loss: 0.3210 - val_acc: 0.8794\n",
            "Epoch 10/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.2876 - acc: 0.8863\n",
            "Epoch 00010: val_acc did not improve from 0.87944\n",
            "51/51 [==============================] - 4s 76ms/step - loss: 0.2876 - acc: 0.8863 - val_loss: 0.3097 - val_acc: 0.8771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvTaRY86wW7S",
        "outputId": "8cf040ed-8b97-4106-b12f-810d17633103"
      },
      "source": [
        "model = load_model('/content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h')\n",
        "\n",
        "\n",
        "\n",
        "###########################################RETREIVING THE TOKENIZER#########################################################\n",
        "\n",
        "# loading\n",
        "with open('/content/drive/MyDrive/NLP-FND/NLP-Fnd/Unzipped/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "    x_test_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_test ) , maxlen =50) #1000 )\n",
        "    x_val_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_val ) , maxlen = 50)#1000 )\n",
        "    x_train_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_train ) , maxlen = 50) #1000 )\n",
        "\n",
        "#############################################################################################################################\n",
        "\n",
        "\n",
        "#evaluation \n",
        "_,train_acc = model.evaluate(x_train_sq_pd,y_train, batch_size=128)\n",
        "_,val_acc = model.evaluate( x_val_sq_pd , y_val , batch_size = 128 )\n",
        "# _,test_acc = model.evaluate( x_test_sq_pd , y_test , batch_size=128 )\n",
        "# print(\"train acc \",test_acc)\n",
        "print(\"val acc \",val_acc)\n",
        "print(\"train acc \",train_acc)\n",
        "# print(\"test acc \",test_acc)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51/51 [==============================] - 1s 18ms/step - loss: 0.2901 - acc: 0.8872\n",
            "17/17 [==============================] - 0s 15ms/step - loss: 0.3210 - acc: 0.8794\n",
            "val acc  0.8794392347335815\n",
            "train acc  0.8872274160385132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0pKUNPuMnO1",
        "outputId": "83b179eb-36fe-4a86-8a4a-e3659afef1de"
      },
      "source": [
        "print(\"val acc \",val_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val acc  0.8873831629753113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PRcsyO8rLbb"
      },
      "source": [
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGIF2HWSwW7S"
      },
      "source": [
        "# DEEP VANILA RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ul2PSNNwW7S",
        "outputId": "5f061e2f-e446-4e86-8f17-39ca2e9fe06b"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(size_of_vocabulary_dataset,300,weights=[embedding_matrix],input_length= threshold ,trainable=False)) \n",
        "model.add( SimpleRNN ( 64 , activation = 'tanh' , return_sequences = True ) )\n",
        "model.add( SimpleRNN(64,activation = 'tanh' , return_sequences = False ))\n",
        "\n",
        "model.add( Dense(32,activation = 'sigmoid' ) )\n",
        "model.add( Dense( 1 , activation = 'sigmoid' ) )\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'] )\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('/content/drive/MyDrive/Models To Save/best_model_deep_VRNN_title.h', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 300)           231949500 \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 50, 64)            23360     \n",
            "_________________________________________________________________\n",
            "simple_rnn_2 (SimpleRNN)     (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 231,983,229\n",
            "Trainable params: 33,729\n",
            "Non-trainable params: 231,949,500\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b64KMr9wW7S",
        "outputId": "4a6bdd55-0316-44d7-f529-59950ca5b6ea"
      },
      "source": [
        "history = model.fit(numpy.array(x_train_sq_pd),numpy.array(y_train),batch_size=128,epochs=10,validation_data=(numpy.array(x_val_sq_pd),numpy.array(y_val)),verbose=1,callbacks=[es,mc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.4672 - acc: 0.7833\n",
            "Epoch 00001: val_acc improved from -inf to 0.86168, saving model to /content/drive/MyDrive/Models To Save/best_model_deep_VRNN_title.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_deep_VRNN_title.h/assets\n",
            "51/51 [==============================] - 16s 308ms/step - loss: 0.4672 - acc: 0.7833 - val_loss: 0.3461 - val_acc: 0.8617\n",
            "Epoch 2/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.3174 - acc: 0.8791\n",
            "Epoch 00002: val_acc improved from 0.86168 to 0.87150, saving model to /content/drive/MyDrive/Models To Save/best_model_deep_VRNN_title.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_deep_VRNN_title.h/assets\n",
            "51/51 [==============================] - 23s 450ms/step - loss: 0.3174 - acc: 0.8791 - val_loss: 0.3109 - val_acc: 0.8715\n",
            "Epoch 3/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.2813 - acc: 0.8914\n",
            "Epoch 00003: val_acc improved from 0.87150 to 0.89860, saving model to /content/drive/MyDrive/Models To Save/best_model_deep_VRNN_title.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_deep_VRNN_title.h/assets\n",
            "51/51 [==============================] - 21s 405ms/step - loss: 0.2813 - acc: 0.8914 - val_loss: 0.2781 - val_acc: 0.8986\n",
            "Epoch 4/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.2402 - acc: 0.9154\n",
            "Epoch 00004: val_acc did not improve from 0.89860\n",
            "51/51 [==============================] - 7s 140ms/step - loss: 0.2402 - acc: 0.9154 - val_loss: 0.2749 - val_acc: 0.8911\n",
            "Epoch 5/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.2173 - acc: 0.9227\n",
            "Epoch 00005: val_acc did not improve from 0.89860\n",
            "51/51 [==============================] - 6s 123ms/step - loss: 0.2173 - acc: 0.9227 - val_loss: 0.2723 - val_acc: 0.8930\n",
            "Epoch 6/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.1854 - acc: 0.9405\n",
            "Epoch 00006: val_acc did not improve from 0.89860\n",
            "51/51 [==============================] - 7s 139ms/step - loss: 0.1854 - acc: 0.9405 - val_loss: 0.2793 - val_acc: 0.8879\n",
            "Epoch 7/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.1640 - acc: 0.9498\n",
            "Epoch 00007: val_acc improved from 0.89860 to 0.89953, saving model to /content/drive/MyDrive/Models To Save/best_model_deep_VRNN_title.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_deep_VRNN_title.h/assets\n",
            "51/51 [==============================] - 20s 397ms/step - loss: 0.1640 - acc: 0.9498 - val_loss: 0.2872 - val_acc: 0.8995\n",
            "Epoch 8/10\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.1382 - acc: 0.9587\n",
            "Epoch 00008: val_acc did not improve from 0.89953\n",
            "51/51 [==============================] - 6s 113ms/step - loss: 0.1382 - acc: 0.9587 - val_loss: 0.3007 - val_acc: 0.8883\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChZ2xK-awW7S",
        "outputId": "0aab8e55-8a66-4245-96a3-6a092d22fcc2"
      },
      "source": [
        "model = load_model('/content/drive/MyDrive/Models To Save/best_model_deep_VRNN_title.h')\n",
        "\n",
        "\n",
        "###########################################RETREIVING THE TOKENIZER#########################################################\n",
        "\n",
        "# loading\n",
        "with open('/content/drive/MyDrive/NLP-FND/NLP-Fnd/Unzipped/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "    x_test_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_test ) , maxlen = 50)# 1000 )\n",
        "    x_val_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_val ) , maxlen = 50)# 1000 )\n",
        "    x_train_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_train ) , maxlen = 50)# 1000 )\n",
        "\n",
        "#############################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#evaluation \n",
        "_,train_acc = model.evaluate(x_train_sq_pd,y_train, batch_size=128)\n",
        "_,val_acc = model.evaluate( x_val_sq_pd , y_val , batch_size = 128 )\n",
        "# _,test_acc = model.evaluate( x_test_sq_pd , y_test , batch_size=128 )\n",
        "print(\"train acc \",train_acc)\n",
        "print(\"val acc \",val_acc)\n",
        "# print(\"test acc \",test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "51/51 [==============================] - 1s 20ms/step - loss: 0.1328 - acc: 0.9611\n",
            "17/17 [==============================] - 0s 21ms/step - loss: 0.2872 - acc: 0.8995\n",
            "train acc  0.9610592126846313\n",
            "val acc  0.8995327353477478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQgxfROMwW7S"
      },
      "source": [
        "# shallow GRU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ahAgy7gwW7S",
        "outputId": "bd732a93-47b6-492c-f41b-d794a5321193"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(size_of_vocabulary_dataset,300,weights=[embedding_matrix],input_length= threshold ,trainable=False)) \n",
        "\n",
        "model.add(GRU(64,return_sequences=False,dropout=0.2))\n",
        "#model.add(GlobalMaxPooling1D())\n",
        "\n",
        "#model.add(Dense(64,activation='relu')) \n",
        "model.add(Dense(1,activation='softmax')) \n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'] )\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('/content/drive/MyDrive/Models To Save/best_model_shallow_GRU_combo.h', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1000, 300)         231949500 \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 64)                70272     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 232,019,837\n",
            "Trainable params: 70,337\n",
            "Non-trainable params: 231,949,500\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e99VnSRswW7S",
        "outputId": "1026dc04-6859-4b34-a7a7-f21b1bebc51c"
      },
      "source": [
        "history = model.fit(numpy.array(x_train_sq_pd),numpy.array(y_train),batch_size=128,epochs=10,validation_data=(numpy.array(x_val_sq_pd),numpy.array(y_val)),verbose=1,callbacks=[es,mc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 7.9749 - acc: 0.4770\n",
            "Epoch 00001: val_acc improved from -inf to 0.47699, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_GRU_combo.h\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_GRU_combo.h/assets\n",
            "197/197 [==============================] - 29s 149ms/step - loss: 7.9749 - acc: 0.4770 - val_loss: 7.9756 - val_acc: 0.4770\n",
            "Epoch 2/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 7.9749 - acc: 0.4770\n",
            "Epoch 00002: val_acc did not improve from 0.47699\n",
            "197/197 [==============================] - 23s 118ms/step - loss: 7.9749 - acc: 0.4770 - val_loss: 7.9756 - val_acc: 0.4770\n",
            "Epoch 3/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 7.9749 - acc: 0.4770\n",
            "Epoch 00003: val_acc did not improve from 0.47699\n",
            "197/197 [==============================] - 22s 113ms/step - loss: 7.9749 - acc: 0.4770 - val_loss: 7.9756 - val_acc: 0.4770\n",
            "Epoch 4/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 7.9749 - acc: 0.4770\n",
            "Epoch 00004: val_acc did not improve from 0.47699\n",
            "197/197 [==============================] - 21s 108ms/step - loss: 7.9749 - acc: 0.4770 - val_loss: 7.9756 - val_acc: 0.4770\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez0x0ONAwW7T",
        "outputId": "81840e0e-b272-4368-b0fd-6f9dda67d92c"
      },
      "source": [
        "model = load_model('/content/drive/MyDrive/NLP-FND/NLP-Fnd/best_model_shallow_GRU_text.h')\n",
        "\n",
        "\n",
        "###########################################RETREIVING THE TOKENIZER#########################################################\n",
        "\n",
        "# loading\n",
        "with open('/content/drive/MyDrive/NLP-FND/NLP-Fnd/Unzipped/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "    x_test_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_test ) , maxlen =1435)# 1000 ) 1000 )\n",
        "    x_val_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_val ) , maxlen = 1435)# 1000 )1000 )\n",
        "    x_train_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_train ) , maxlen = 1435)# 1000 )1000 )\n",
        "\n",
        "#############################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#evaluation \n",
        "_,train_acc = model.evaluate(x_train_sq_pd,y_train, batch_size=128)\n",
        "_,val_acc = model.evaluate( x_val_sq_pd , y_val , batch_size = 128 )\n",
        "# _,test_acc = model.evaluate( x_test_sq_pd , y_test , batch_size=128 )\n",
        "print(\"train acc \",train_acc)\n",
        "print(\"val acc \",val_acc)\n",
        "# print(\"test acc \",test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "350/350 [==============================] - 21s 60ms/step - loss: 7.6246 - acc: 0.5000\n",
            "150/150 [==============================] - 9s 60ms/step - loss: 7.6246 - acc: 0.5000\n",
            "125/125 [==============================] - 7s 60ms/step - loss: 7.6246 - acc: 0.5000\n",
            "train acc  0.5\n",
            "val acc  0.5\n",
            "test acc  0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJw1KCQfwW7T",
        "outputId": "93113a69-d468-441e-8926-6aed1b648923"
      },
      "source": [
        "train_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ9F9S0YwW7T"
      },
      "source": [
        "# deep GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FGBzxW8wW7T",
        "outputId": "0652cfbb-7d50-4bc6-bb0b-660682720d27"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(size_of_vocabulary_dataset,300,weights=[embedding_matrix],input_length= threshold ,trainable=False)) \n",
        "\n",
        "model.add(GRU(64,return_sequences=True,dropout=0.2))\n",
        "model.add ( GRU(64,return_sequences=False, dropout=0.2) )\n",
        "\n",
        "model.add(Dense(32,activation='softmax')) \n",
        "model.add(Dense(1,activation='softmax'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'] )\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('/content/drive/MyDrive/Models To Save/best_model_deep_GRU_title.h', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1000, 300)         231949500 \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 1000, 64)          70272     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 64)                24960     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 232,046,845\n",
            "Trainable params: 97,345\n",
            "Non-trainable params: 231,949,500\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9g5FLNvwW7T",
        "outputId": "f9bfa10b-64b1-4581-b94c-32ecfb48da75"
      },
      "source": [
        "history = model.fit(numpy.array(x_train_sq_pd),numpy.array(y_train),batch_size=128,epochs=10,validation_data=(numpy.array(x_val_sq_pd),numpy.array(y_val)),verbose=1,callbacks=[es,mc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 7.9749 - acc: 0.4770\n",
            "Epoch 00001: val_acc improved from -inf to 0.47699, saving model to /content/drive/MyDrive/Models To Save/best_model_deep_GRU_title.h\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_deep_GRU_title.h/assets\n",
            "197/197 [==============================] - 42s 213ms/step - loss: 7.9749 - acc: 0.4770 - val_loss: 7.9756 - val_acc: 0.4770\n",
            "Epoch 2/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 7.9749 - acc: 0.4770\n",
            "Epoch 00002: val_acc did not improve from 0.47699\n",
            "197/197 [==============================] - 31s 156ms/step - loss: 7.9749 - acc: 0.4770 - val_loss: 7.9756 - val_acc: 0.4770\n",
            "Epoch 3/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 7.9749 - acc: 0.4770\n",
            "Epoch 00003: val_acc did not improve from 0.47699\n",
            "197/197 [==============================] - 30s 151ms/step - loss: 7.9749 - acc: 0.4770 - val_loss: 7.9756 - val_acc: 0.4770\n",
            "Epoch 4/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 7.9749 - acc: 0.4770\n",
            "Epoch 00004: val_acc did not improve from 0.47699\n",
            "197/197 [==============================] - 30s 151ms/step - loss: 7.9749 - acc: 0.4770 - val_loss: 7.9756 - val_acc: 0.4770\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg9F-az9wW7T",
        "outputId": "80ff2260-ccca-43b8-c77d-073d6eb62d92"
      },
      "source": [
        "model = load_model('/content/drive/MyDrive/Models To Save/best_model_deep_GRU_title.h')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###########################################RETREIVING THE TOKENIZER#########################################################\n",
        "\n",
        "# loading\n",
        "with open('/content/drive/MyDrive/NLP-FND/NLP-Fnd/Unzipped/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "    x_test_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_test ) , maxlen = 1435) # 1000 ) 1000 )\n",
        "    x_val_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_val ) , maxlen = 1435)# 1000 )1000 )\n",
        "    x_train_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_train ) , maxlen = 1435)# 1000 )1000 )\n",
        "\n",
        "#############################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "#evaluation \n",
        "_,train_acc = model.evaluate(x_train_sq_pd,y_train, batch_size=128)\n",
        "_,val_acc = model.evaluate( x_val_sq_pd , y_val , batch_size = 128 )\n",
        "# _,test_acc = model.evaluate( x_test_sq_pd , y_test , batch_size=128 )\n",
        "print(\"train acc \",train_acc)\n",
        "print(\"val acc \",val_acc)\n",
        "# print(\"test acc \",test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "197/197 [==============================] - 15s 74ms/step - loss: 7.9749 - acc: 0.4770\n",
            "85/85 [==============================] - 6s 73ms/step - loss: 7.9756 - acc: 0.4770\n",
            "71/71 [==============================] - 5s 73ms/step - loss: 7.9752 - acc: 0.4770\n",
            "train acc  0.47702956199645996\n",
            "val acc  0.476985901594162\n",
            "test acc  0.4770070016384125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05gm8OH9wW7U"
      },
      "source": [
        "# shallow LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RmUCt6EwW7U",
        "outputId": "1709a7fe-0586-43da-c89a-2d52895867f6"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(size_of_vocabulary_dataset,300,weights=[embedding_matrix],input_length= threshold ,trainable=False)) \n",
        "\n",
        "model.add ( LSTM(64,return_sequences=False) )\n",
        "\n",
        "model.add(Dense(32,activation='sigmoid')) \n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'] )\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('/content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1000, 300)         231949500 \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                93440     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 232,045,053\n",
            "Trainable params: 95,553\n",
            "Non-trainable params: 231,949,500\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7pYnwoPwW7U",
        "scrolled": true,
        "outputId": "2a0d15c7-339a-48f7-af5a-07af854c15d8"
      },
      "source": [
        "history = model.fit(numpy.array(x_train_sq_pd),numpy.array(y_train),batch_size=128,epochs=10,validation_data=(numpy.array(x_val_sq_pd),numpy.array(y_val)),verbose=1,callbacks=[es,mc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.2693 - acc: 0.8908\n",
            "Epoch 00001: val_acc improved from -inf to 0.93597, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h/assets\n",
            "197/197 [==============================] - 30s 150ms/step - loss: 0.2693 - acc: 0.8908 - val_loss: 0.1673 - val_acc: 0.9360\n",
            "Epoch 2/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1519 - acc: 0.9430\n",
            "Epoch 00002: val_acc improved from 0.93597 to 0.94209, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h/assets\n",
            "197/197 [==============================] - 30s 154ms/step - loss: 0.1519 - acc: 0.9430 - val_loss: 0.1493 - val_acc: 0.9421\n",
            "Epoch 3/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1371 - acc: 0.9492\n",
            "Epoch 00003: val_acc improved from 0.94209 to 0.94618, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h/assets\n",
            "197/197 [==============================] - 30s 150ms/step - loss: 0.1371 - acc: 0.9492 - val_loss: 0.1452 - val_acc: 0.9462\n",
            "Epoch 4/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1183 - acc: 0.9549\n",
            "Epoch 00004: val_acc did not improve from 0.94618\n",
            "197/197 [==============================] - 22s 111ms/step - loss: 0.1183 - acc: 0.9549 - val_loss: 0.1448 - val_acc: 0.9432\n",
            "Epoch 5/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.1066 - acc: 0.9607\n",
            "Epoch 00005: val_acc improved from 0.94618 to 0.94924, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h/assets\n",
            "197/197 [==============================] - 30s 152ms/step - loss: 0.1066 - acc: 0.9607 - val_loss: 0.1354 - val_acc: 0.9492\n",
            "Epoch 6/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.0915 - acc: 0.9677\n",
            "Epoch 00006: val_acc improved from 0.94924 to 0.95174, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h/assets\n",
            "197/197 [==============================] - 30s 150ms/step - loss: 0.0915 - acc: 0.9677 - val_loss: 0.1274 - val_acc: 0.9517\n",
            "Epoch 7/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.0807 - acc: 0.9716\n",
            "Epoch 00007: val_acc improved from 0.95174 to 0.95267, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h/assets\n",
            "197/197 [==============================] - 31s 156ms/step - loss: 0.0807 - acc: 0.9716 - val_loss: 0.1288 - val_acc: 0.9527\n",
            "Epoch 8/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.0713 - acc: 0.9753\n",
            "Epoch 00008: val_acc improved from 0.95267 to 0.95490, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h/assets\n",
            "197/197 [==============================] - 30s 154ms/step - loss: 0.0713 - acc: 0.9753 - val_loss: 0.1249 - val_acc: 0.9549\n",
            "Epoch 9/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.0592 - acc: 0.9794\n",
            "Epoch 00009: val_acc did not improve from 0.95490\n",
            "197/197 [==============================] - 22s 110ms/step - loss: 0.0592 - acc: 0.9794 - val_loss: 0.1357 - val_acc: 0.9530\n",
            "Epoch 10/10\n",
            "197/197 [==============================] - ETA: 0s - loss: 0.0531 - acc: 0.9821\n",
            "Epoch 00010: val_acc improved from 0.95490 to 0.95611, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h/assets\n",
            "197/197 [==============================] - 29s 147ms/step - loss: 0.0531 - acc: 0.9821 - val_loss: 0.1391 - val_acc: 0.9561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZmWlWk-wW7U",
        "outputId": "548ee608-00c3-4931-966a-b71036d52ad9"
      },
      "source": [
        "model = load_model('/content/drive/MyDrive/Models To Save/best_model_shallow_LSTM_title.h')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###########################################RETREIVING THE TOKENIZER#########################################################\n",
        "\n",
        "# loading\n",
        "with open('/content/drive/MyDrive/NLP-FND/NLP-Fnd/Unzipped/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "    x_test_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_test ) , maxlen = 1435)# 1000 )1000 )\n",
        "    x_val_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_val ) , maxlen = 1435)# 1000 )1000 )\n",
        "    x_train_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_train ) , maxlen =1435)# 1000 ) 1000 )\n",
        "\n",
        "#############################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#evaluation \n",
        "_,train_acc = model.evaluate(x_train_sq_pd,y_train, batch_size=128)\n",
        "_,val_acc = model.evaluate( x_val_sq_pd , y_val , batch_size = 128 )\n",
        "# _,test_acc = model.evaluate( x_test_sq_pd , y_test , batch_size=128 )\n",
        "print(\"train acc \",train_acc)\n",
        "print(\"val acc \",val_acc)\n",
        "# print(\"test acc \",test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "197/197 [==============================] - 12s 61ms/step - loss: 0.0433 - acc: 0.9864\n",
            "85/85 [==============================] - 5s 60ms/step - loss: 0.1391 - acc: 0.9561\n",
            "71/71 [==============================] - 4s 59ms/step - loss: 0.1391 - acc: 0.9529\n",
            "train acc  0.9863569736480713\n",
            "val acc  0.9561061859130859\n",
            "test acc  0.9529005885124207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W90SM6AXNO7e"
      },
      "source": [
        "#Label Prediction\n",
        "y_testss_predictss= model.predict(x_test_sq_pd)\n",
        "# predicted=np.concatenate((trainPredict,testPredict),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo4U_8-YNWCO",
        "outputId": "4cfe5f25-4795-425b-d8ee-eb6ce5a15723"
      },
      "source": [
        "y_testss_predictss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.2582101e-04],\n",
              "       [3.3134528e-04],\n",
              "       [2.9919305e-04],\n",
              "       ...,\n",
              "       [9.7824210e-01],\n",
              "       [9.9845457e-01],\n",
              "       [1.1021795e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybjHcGPPwW7V"
      },
      "source": [
        "# deep LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAG4imjZwW7V",
        "outputId": "937358ac-da23-4ebb-c758-ea7d6ff78c7a"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(size_of_vocabulary_dataset,300,weights=[embedding_matrix],input_length= threshold ,trainable=False)) \n",
        "\n",
        "model.add ( LSTM(64,return_sequences=True) )\n",
        "model.add( LSTM(64,return_sequences=False) )\n",
        "\n",
        "model.add(Dense(32,activation='sigmoid')) \n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'] )\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('/content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 300)           231949500 \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50, 64)            93440     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 232,078,077\n",
            "Trainable params: 128,577\n",
            "Non-trainable params: 231,949,500\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjwnQlqbwW7V",
        "outputId": "0746f595-fef7-4f2f-983e-744baf659a29"
      },
      "source": [
        "history = model.fit(numpy.array(x_train_sq_pd),numpy.array(y_train),batch_size=128,epochs=20,validation_data=(numpy.array(x_val_sq_pd),numpy.array(y_val)),verbose=1,callbacks=[es,mc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "50/53 [===========================>..] - ETA: 0s - loss: 0.5031 - acc: 0.7561\n",
            "Epoch 00001: val_acc improved from -inf to 0.87076, saving model to /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h/assets\n",
            "53/53 [==============================] - 14s 258ms/step - loss: 0.4956 - acc: 0.7609 - val_loss: 0.3338 - val_acc: 0.8708\n",
            "Epoch 2/20\n",
            "52/53 [============================>.] - ETA: 0s - loss: 0.3065 - acc: 0.8773\n",
            "Epoch 00002: val_acc improved from 0.87076 to 0.89279, saving model to /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h/assets\n",
            "53/53 [==============================] - 12s 224ms/step - loss: 0.3056 - acc: 0.8778 - val_loss: 0.2750 - val_acc: 0.8928\n",
            "Epoch 3/20\n",
            "53/53 [==============================] - ETA: 0s - loss: 0.2537 - acc: 0.9013\n",
            "Epoch 00003: val_acc improved from 0.89279 to 0.90232, saving model to /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h/assets\n",
            "53/53 [==============================] - 27s 518ms/step - loss: 0.2537 - acc: 0.9013 - val_loss: 0.2618 - val_acc: 0.9023\n",
            "Epoch 4/20\n",
            "50/53 [===========================>..] - ETA: 0s - loss: 0.2242 - acc: 0.9133\n",
            "Epoch 00004: val_acc improved from 0.90232 to 0.90590, saving model to /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h/assets\n",
            "53/53 [==============================] - 22s 418ms/step - loss: 0.2240 - acc: 0.9138 - val_loss: 0.2342 - val_acc: 0.9059\n",
            "Epoch 5/20\n",
            "50/53 [===========================>..] - ETA: 0s - loss: 0.2013 - acc: 0.9219\n",
            "Epoch 00005: val_acc did not improve from 0.90590\n",
            "53/53 [==============================] - 1s 14ms/step - loss: 0.2001 - acc: 0.9226 - val_loss: 0.2634 - val_acc: 0.9011\n",
            "Epoch 6/20\n",
            "50/53 [===========================>..] - ETA: 0s - loss: 0.1876 - acc: 0.9306\n",
            "Epoch 00006: val_acc improved from 0.90590 to 0.91185, saving model to /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h/assets\n",
            "53/53 [==============================] - 22s 418ms/step - loss: 0.1878 - acc: 0.9312 - val_loss: 0.2365 - val_acc: 0.9119\n",
            "Epoch 7/20\n",
            "51/53 [===========================>..] - ETA: 0s - loss: 0.1621 - acc: 0.9422\n",
            "Epoch 00007: val_acc improved from 0.91185 to 0.91960, saving model to /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h/assets\n",
            "53/53 [==============================] - 25s 465ms/step - loss: 0.1641 - acc: 0.9419 - val_loss: 0.2301 - val_acc: 0.9196\n",
            "Epoch 8/20\n",
            "51/53 [===========================>..] - ETA: 0s - loss: 0.1548 - acc: 0.9439\n",
            "Epoch 00008: val_acc did not improve from 0.91960\n",
            "53/53 [==============================] - 1s 16ms/step - loss: 0.1562 - acc: 0.9436 - val_loss: 0.2340 - val_acc: 0.9148\n",
            "Epoch 9/20\n",
            "53/53 [==============================] - ETA: 0s - loss: 0.1406 - acc: 0.9525\n",
            "Epoch 00009: val_acc did not improve from 0.91960\n",
            "53/53 [==============================] - 1s 14ms/step - loss: 0.1406 - acc: 0.9525 - val_loss: 0.2280 - val_acc: 0.9178\n",
            "Epoch 10/20\n",
            "52/53 [============================>.] - ETA: 0s - loss: 0.1237 - acc: 0.9602\n",
            "Epoch 00010: val_acc did not improve from 0.91960\n",
            "53/53 [==============================] - 1s 14ms/step - loss: 0.1231 - acc: 0.9605 - val_loss: 0.2545 - val_acc: 0.9190\n",
            "Epoch 11/20\n",
            "49/53 [==========================>...] - ETA: 0s - loss: 0.1272 - acc: 0.9570\n",
            "Epoch 00011: val_acc did not improve from 0.91960\n",
            "53/53 [==============================] - 1s 14ms/step - loss: 0.1236 - acc: 0.9583 - val_loss: 0.2367 - val_acc: 0.9196\n",
            "Epoch 12/20\n",
            "50/53 [===========================>..] - ETA: 0s - loss: 0.1065 - acc: 0.9667\n",
            "Epoch 00012: val_acc did not improve from 0.91960\n",
            "53/53 [==============================] - 1s 14ms/step - loss: 0.1073 - acc: 0.9666 - val_loss: 0.2494 - val_acc: 0.9160\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQCowIPIwW7W",
        "outputId": "81dbf9ae-787c-4fe2-c70b-78628edc4c58"
      },
      "source": [
        "model = load_model('/content/drive/MyDrive/Models To Save/best_model_deep_LSTM_combos.h')\n",
        "\n",
        "\n",
        "###########################################RETREIVING THE TOKENIZER#########################################################\n",
        "\n",
        "# loading\n",
        "with open('/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Unzipped/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "    x_test_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_test ) , maxlen =50)# 1000 ) 1000 )\n",
        "    x_val_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_val ) , maxlen =50)# 1000 ) 1000 )\n",
        "    x_train_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_train ) , maxlen = 50)# 1000 )1000 )\n",
        "\n",
        "#############################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#evaluation \n",
        "_,train_acc = model.evaluate(x_train_sq_pd,y_train, batch_size=128)\n",
        "_,val_acc = model.evaluate( x_val_sq_pd , y_val , batch_size = 128 )\n",
        "# _,test_acc = model.evaluate( x_test_sq_pd , y_test , batch_size=128 )\n",
        "print(\"train acc \",train_acc)\n",
        "print(\"val acc \",val_acc)\n",
        "# print(\"test acc \",test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53/53 [==============================] - 0s 7ms/step - loss: 0.1387 - acc: 0.9532\n",
            "14/14 [==============================] - 0s 7ms/step - loss: 0.2301 - acc: 0.9196\n",
            "train acc  0.953245997428894\n",
            "val acc  0.9195950031280518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w99MV4iGpmjr"
      },
      "source": [
        "#**CNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nm86Ek2pY_d"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(size_of_vocabulary_dataset, 300,weights=[embedding_matrix],input_length= threshold ,trainable=False))\n",
        "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add( Dense( 1 , activation = 'sigmoid' ) )\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'] )\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "mc=ModelCheckpoint('/content/drive/MyDrive/Models To Save/model_CNN_combine.h', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPw7CDlqpZRy"
      },
      "source": [
        "history = model.fit(numpy.array(x_train_sq_pd),numpy.array(y_train),batch_size=128,epochs=10,validation_data=(numpy.array(x_val_sq_pd),numpy.array(y_val)),verbose=1,callbacks=[es,mc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBi8ID6TpZbM",
        "outputId": "81f32d12-ef95-4577-c688-47a4dfc5b01a"
      },
      "source": [
        "model = load_model('/content/drive/MyDrive/Models To Save/model_CNN_combine.h')\n",
        "\n",
        "\n",
        "###########################################RETREIVING THE TOKENIZER#########################################################\n",
        "\n",
        "# loading\n",
        "with open('/content/drive/MyDrive/NLP-FND/NLP-Fnd/Unzipped/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "    x_test_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_test ) , maxlen =1435)# 1000 ) 1000 )\n",
        "    x_val_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_val ) , maxlen = 1435)# 1000 )1000 )\n",
        "    x_train_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_train ) , maxlen =1435)# 1000 ) 1000 )\n",
        "\n",
        "#############################################################################################################################\n",
        "\n",
        "\n",
        "#evaluation \n",
        "_,train_acc = model.evaluate(x_train_sq_pd,y_train, batch_size=128)\n",
        "_,val_acc = model.evaluate( x_val_sq_pd , y_val , batch_size = 128 )\n",
        "# _,test_acc = model.evaluate( x_test_sq_pd , y_test , batch_size=128 )\n",
        "print(\"train acc \",test_acc)\n",
        "print(\"val acc \",val_acc)\n",
        "# print(\"test acc \",test_acc)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "197/197 [==============================] - 8s 42ms/step - loss: 0.0017 - acc: 1.0000\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.0320 - acc: 0.9907\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.0292 - acc: 0.9916\n",
            "train acc  0.9916490316390991\n",
            "val acc  0.9907200932502747\n",
            "test acc  0.9916490316390991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL3MGbHbOtY0",
        "outputId": "5076f408-873c-4d83-fae4-12f7a20cd6ea"
      },
      "source": [
        "history = model.fit(numpy.array(x_train_sq_pd),numpy.array(y_train),batch_size=128,epochs=10,validation_data=(numpy.array(x_val_sq_pd),numpy.array(y_val)),verbose=1,callbacks=[es,mc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.5607 - acc: 0.7104\n",
            "Epoch 00001: val_acc did not improve from 0.87541\n",
            "29/29 [==============================] - 21s 737ms/step - loss: 0.5607 - acc: 0.7104 - val_loss: 0.4348 - val_acc: 0.8099\n",
            "Epoch 2/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.3617 - acc: 0.8609\n",
            "Epoch 00002: val_acc did not improve from 0.87541\n",
            "29/29 [==============================] - 20s 686ms/step - loss: 0.3617 - acc: 0.8609 - val_loss: 0.3299 - val_acc: 0.8663\n",
            "Epoch 3/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.3146 - acc: 0.8787\n",
            "Epoch 00003: val_acc improved from 0.87541 to 0.87800, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "29/29 [==============================] - 53s 2s/step - loss: 0.3146 - acc: 0.8787 - val_loss: 0.3047 - val_acc: 0.8780\n",
            "Epoch 4/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.2739 - acc: 0.8962\n",
            "Epoch 00004: val_acc improved from 0.87800 to 0.89098, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "29/29 [==============================] - 59s 2s/step - loss: 0.2739 - acc: 0.8962 - val_loss: 0.2862 - val_acc: 0.8910\n",
            "Epoch 5/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.2451 - acc: 0.9079\n",
            "Epoch 00005: val_acc improved from 0.89098 to 0.89163, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "29/29 [==============================] - 65s 2s/step - loss: 0.2451 - acc: 0.9079 - val_loss: 0.2790 - val_acc: 0.8916\n",
            "Epoch 6/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.2136 - acc: 0.9213\n",
            "Epoch 00006: val_acc improved from 0.89163 to 0.89487, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "29/29 [==============================] - 67s 2s/step - loss: 0.2136 - acc: 0.9213 - val_loss: 0.2589 - val_acc: 0.8949\n",
            "Epoch 7/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.1981 - acc: 0.9293\n",
            "Epoch 00007: val_acc did not improve from 0.89487\n",
            "29/29 [==============================] - 22s 752ms/step - loss: 0.1981 - acc: 0.9293 - val_loss: 0.3230 - val_acc: 0.8728\n",
            "Epoch 8/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.1957 - acc: 0.9305\n",
            "Epoch 00008: val_acc improved from 0.89487 to 0.89942, saving model to /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models To Save/best_model_shallow_VRNN_combine.h/assets\n",
            "29/29 [==============================] - 57s 2s/step - loss: 0.1957 - acc: 0.9305 - val_loss: 0.2881 - val_acc: 0.8994\n",
            "Epoch 9/10\n",
            "29/29 [==============================] - ETA: 0s - loss: 0.1743 - acc: 0.9377\n",
            "Epoch 00009: val_acc did not improve from 0.89942\n",
            "29/29 [==============================] - 20s 687ms/step - loss: 0.1743 - acc: 0.9377 - val_loss: 0.2641 - val_acc: 0.8988\n",
            "Epoch 00009: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHHzFmV-pZZY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27QcOJw-pZWT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0YnNBq_4-LH"
      },
      "source": [
        "#**Bidirectional LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN8T1LtF47f8",
        "outputId": "fdfa44e0-9b2f-4f99-bcb6-affb4b431bee"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.add(Embedding(size_of_vocabulary_dataset,300,weights=[embedding_matrix],input_length= threshold ,trainable=False)) \n",
        "\n",
        "model.add ( Bidirectional(LSTM(64,return_sequences=True) ))\n",
        "model.add( Bidirectional( LSTM(64,return_sequences=False) ))\n",
        "\n",
        "model.add(Dense(32,activation='sigmoid')) \n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'] )\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \n",
        "history=ModelCheckpoint('/content/drive/MyDrive/Models To Save/model_CNN_combine.h', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n",
        "\n",
        "#Print summary of model\n",
        "print(model.summary())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 1024, 300)         231949500 \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 1024, 128)         186880    \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 232,239,357\n",
            "Trainable params: 289,857\n",
            "Non-trainable params: 231,949,500\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "wwopty1U5eUu",
        "outputId": "e78ed385-5142-4d53-f473-24238454cb4c"
      },
      "source": [
        "model = load_model('/content/drive/MyDrive/Models To Save/best_model_bidirectional_deep_LSTM_combos.h')\n",
        "\n",
        "\n",
        "\n",
        "###########################################RETREIVING THE TOKENIZER#########################################################\n",
        "\n",
        "# loading\n",
        "with open('/content/drive/MyDrive/NLP-Fnd/NLP-Fnd/Unzipped/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "    x_test_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_test ) , maxlen = 1435)# 1000 ) 1000 )\n",
        "    x_val_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_val ) , maxlen =  1435)# 1000 ) 1000 )\n",
        "    x_train_sq_pd= pad_sequences( tokenizer.texts_to_sequences( x_train ) , maxlen = 1435)# 1000 ) 1000 )\n",
        "\n",
        "#############################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#evaluation \n",
        "_,train_acc = model.evaluate(x_train_sq_pd,y_train, batch_size=128)\n",
        "_,val_acc = model.evaluate( x_val_sq_pd , y_val , batch_size = 128 )\n",
        "# _,test_acc = model.evaluate( x_test_sq_pd , y_test , batch_size=128 )\n",
        "print(\"train acc \",train_acc)\n",
        "print(\"val acc \",val_acc)\n",
        "# print(\"test acc \",test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f94bbbe3441d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Models To Save/best_model_bidirectional_deep_LSTM_combos.h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m###########################################RETREIVING THE TOKENIZER#########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dMhid7iD24e"
      },
      "source": [
        "#**Checking The Labels:   0:False  1: True**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wiksWG2Dv8c"
      },
      "source": [
        "#\n",
        "########################################################################################################################\n",
        "#_____________________________________________________Prediction___________________________________________ \n",
        "########################################################################################################################\n",
        "\n",
        "#Label Prediction\n",
        "y_testss_predictss= model.predict(x_test_sq_pd)\n",
        "\n",
        "final_df=pd.DataFrame()\n",
        "final_df[\"id\"]=test_data[\"id\"]\n",
        "final_df[\"label\"]=convertToLabels(y_testss_predictss)\n",
        "\n",
        "final_df.to_csv(r\"/content/drive/MyDrive/DMG_A4/answer_DeepLSTM.txt\",index=None)#None)\n",
        "\n",
        "# convertToLabels()\n",
        "# predicted=np.concatenate((trainPredict,testPredict),axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3GjfOOlsmqF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}